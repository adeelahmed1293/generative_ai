{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the different Format of data like text , csv , url , pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': './data/FAQs.csv', 'row': 0}, page_content='What is the capital of France?: How many continents are there in the world?\\nThe capital of France is Paris.: There are seven continents: Africa, Antarctica, Asia, Europe, North America, Oceania, and South America.'), Document(metadata={'source': './data/FAQs.csv', 'row': 1}, page_content='What is the capital of France?: What is the largest planet in our solar system?\\nThe capital of France is Paris.: Jupiter is the largest planet in our solar system.'), Document(metadata={'source': './data/FAQs.csv', 'row': 2}, page_content='What is the capital of France?: Who discovered gravity?\\nThe capital of France is Paris.: Sir Isaac Newton discovered gravity.'), Document(metadata={'source': './data/FAQs.csv', 'row': 3}, page_content='What is the capital of France?: What are the three states of matter?\\nThe capital of France is Paris.: Solid, liquid, and gas.'), Document(metadata={'source': './data/FAQs.csv', 'row': 4}, page_content=\"What is the capital of France?: Who wrote 'Romeo and Juliet'?\\nThe capital of France is Paris.: William Shakespeare.\"), Document(metadata={'source': './data/FAQs.csv', 'row': 5}, page_content='What is the capital of France?: What is the chemical symbol for water?\\nThe capital of France is Paris.: H2O.'), Document(metadata={'source': './data/FAQs.csv', 'row': 6}, page_content='What is the capital of France?: What is the speed of light?\\nThe capital of France is Paris.: Approximately 299,792,458 meters per second (about 300,000 km/s).'), Document(metadata={'source': './data/FAQs.csv', 'row': 7}, page_content='What is the capital of France?: Who was the first person to walk on the moon?\\nThe capital of France is Paris.: Neil Armstrong in 1969.'), Document(metadata={'source': './data/FAQs.csv', 'row': 8}, page_content='What is the capital of France?: What is artificial intelligence (AI)?\\nThe capital of France is Paris.: AI is the simulation of human intelligence in machines that can think, learn, and solve problems.'), Document(metadata={'source': './data/FAQs.csv', 'row': 9}, page_content='What is the capital of France?: What does CPU stand for?\\nThe capital of France is Paris.: Central Processing Unit.'), Document(metadata={'source': './data/FAQs.csv', 'row': 10}, page_content='What is the capital of France?: What is the difference between RAM and ROM?\\nThe capital of France is Paris.: RAM is temporary memory used for active tasks, while ROM is permanent storage used for system instructions.'), Document(metadata={'source': './data/FAQs.csv', 'row': 11}, page_content='What is the capital of France?: What is an IP address?\\nThe capital of France is Paris.: It is a unique numerical address assigned to a device on a network.'), Document(metadata={'source': './data/FAQs.csv', 'row': 12}, page_content='What is the capital of France?: What does HTTP stand for?\\nThe capital of France is Paris.: Hypertext Transfer Protocol.'), Document(metadata={'source': './data/FAQs.csv', 'row': 13}, page_content='What is the capital of France?: What is cloud computing?\\nThe capital of France is Paris.: Cloud computing is the delivery of computing services over the internet instead of local servers.'), Document(metadata={'source': './data/FAQs.csv', 'row': 14}, page_content='What is the capital of France?: What is an operating system?\\nThe capital of France is Paris.: It is software that manages hardware and software resources on a computer (e.g., Windows, macOS, Linux).'), Document(metadata={'source': './data/FAQs.csv', 'row': 15}, page_content='What is the capital of France?: What does HTML stand for?\\nThe capital of France is Paris.: HyperText Markup Language.'), Document(metadata={'source': './data/FAQs.csv', 'row': 16}, page_content='What is the capital of France?: What is the difference between a website and a web application?\\nThe capital of France is Paris.: A website is informational, while a web application is interactive and allows user input.'), Document(metadata={'source': './data/FAQs.csv', 'row': 17}, page_content='What is the capital of France?: What is cybersecurity?\\nThe capital of France is Paris.: The practice of protecting systems, networks, and data from cyber threats.'), Document(metadata={'source': './data/FAQs.csv', 'row': 18}, page_content='What is the capital of France?: What is DNA?\\nThe capital of France is Paris.: DNA (Deoxyribonucleic Acid) carries genetic information in living organisms.'), Document(metadata={'source': './data/FAQs.csv', 'row': 19}, page_content=\"What is the capital of France?: What causes seasons on Earth?\\nThe capital of France is Paris.: The tilt of the Earth's axis as it orbits the sun.\"), Document(metadata={'source': './data/FAQs.csv', 'row': 20}, page_content='What is the capital of France?: How does the human brain function?\\nThe capital of France is Paris.: It processes information, controls the body, and enables thinking and emotions.'), Document(metadata={'source': './data/FAQs.csv', 'row': 21}, page_content=\"What is the capital of France?: What is the immune system?\\nThe capital of France is Paris.: The body's defense system against infections and diseases.\"), Document(metadata={'source': './data/FAQs.csv', 'row': 22}, page_content='What is the capital of France?: What are the benefits of exercise?\\nThe capital of France is Paris.: Improves heart health, strengthens muscles, boosts mood, and enhances overall well-being.'), Document(metadata={'source': './data/FAQs.csv', 'row': 23}, page_content='What is the capital of France?: How does the heart work?\\nThe capital of France is Paris.: It pumps oxygen-rich blood throughout the body and removes carbon dioxide.'), Document(metadata={'source': './data/FAQs.csv', 'row': 24}, page_content='What is the capital of France?: What are renewable and nonrenewable energy sources?\\nThe capital of France is Paris.: Renewable: Solar, wind, hydro. Nonrenewable: Coal, oil, natural gas.'), Document(metadata={'source': './data/FAQs.csv', 'row': 25}, page_content='What is the capital of France?: What is the greenhouse effect?\\nThe capital of France is Paris.: The trapping of heat in Earth’s atmosphere by greenhouse gases.'), Document(metadata={'source': './data/FAQs.csv', 'row': 26}, page_content='What is the capital of France?: What is a virus?\\nThe capital of France is Paris.: A microscopic infectious agent that replicates inside a host cell.'), Document(metadata={'source': './data/FAQs.csv', 'row': 27}, page_content='What is the capital of France?: Why do we need sleep?\\nThe capital of France is Paris.: For brain function, body repair, and overall health.'), Document(metadata={'source': './data/FAQs.csv', 'row': 28}, page_content='What is the capital of France?: What is inflation?\\nThe capital of France is Paris.: The increase in prices over time, reducing purchasing power.'), Document(metadata={'source': './data/FAQs.csv', 'row': 29}, page_content='What is the capital of France?: How do credit cards work?\\nThe capital of France is Paris.: They allow you to borrow money from a bank for purchases and repay later, often with interest.'), Document(metadata={'source': './data/FAQs.csv', 'row': 30}, page_content='What is the capital of France?: What is cryptocurrency?\\nThe capital of France is Paris.: A digital currency that uses encryption for security (e.g., Bitcoin).'), Document(metadata={'source': './data/FAQs.csv', 'row': 31}, page_content='What is the capital of France?: What is a stock market?\\nThe capital of France is Paris.: A marketplace where stocks are bought and sold.'), Document(metadata={'source': './data/FAQs.csv', 'row': 32}, page_content='What is the capital of France?: What is the difference between debit and credit?\\nThe capital of France is Paris.: Debit withdraws money you own; credit borrows money.'), Document(metadata={'source': './data/FAQs.csv', 'row': 33}, page_content='What is the capital of France?: How do bank loans work?\\nThe capital of France is Paris.: A bank lends you money that must be repaid with interest.'), Document(metadata={'source': './data/FAQs.csv', 'row': 34}, page_content='What is the capital of France?: What is an investment?\\nThe capital of France is Paris.: Allocating money to an asset to gain profit.'), Document(metadata={'source': './data/FAQs.csv', 'row': 35}, page_content='What is the capital of France?: What is compound interest?\\nThe capital of France is Paris.: Interest earned on both the initial principal and accumulated interest.'), Document(metadata={'source': './data/FAQs.csv', 'row': 36}, page_content='What is the capital of France?: What does GDP stand for?\\nThe capital of France is Paris.: Gross Domestic Product, the total value of goods and services in a country.'), Document(metadata={'source': './data/FAQs.csv', 'row': 37}, page_content='What is the capital of France?: What is entrepreneurship?\\nThe capital of France is Paris.: Starting and managing a business venture.'), Document(metadata={'source': './data/FAQs.csv', 'row': 38}, page_content='What is the capital of France?: What are the top tourist destinations in the world?\\nThe capital of France is Paris.: Paris, New York, London, Dubai, Tokyo.'), Document(metadata={'source': './data/FAQs.csv', 'row': 39}, page_content='What is the capital of France?: What are the safest countries to travel to?\\nThe capital of France is Paris.: Switzerland, Canada, Denmark, Iceland.'), Document(metadata={'source': './data/FAQs.csv', 'row': 40}, page_content=\"What is the capital of France?: How do I apply for a passport?\\nThe capital of France is Paris.: Submit an application with required documents to your country's passport office.\"), Document(metadata={'source': './data/FAQs.csv', 'row': 41}, page_content='What is the capital of France?: What is a visa?\\nThe capital of France is Paris.: A permit that allows you to enter a foreign country.'), Document(metadata={'source': './data/FAQs.csv', 'row': 42}, page_content='What is the capital of France?: How can I find cheap flights?\\nThe capital of France is Paris.: Use fare comparison websites, book early, and be flexible with dates.'), Document(metadata={'source': './data/FAQs.csv', 'row': 43}, page_content='What is the capital of France?: What are the most popular sports in the world?\\nThe capital of France is Paris.: Soccer, basketball, cricket, tennis, baseball.'), Document(metadata={'source': './data/FAQs.csv', 'row': 44}, page_content='What is the capital of France?: What is the FIFA World Cup?\\nThe capital of France is Paris.: The biggest international soccer tournament, held every four years.'), Document(metadata={'source': './data/FAQs.csv', 'row': 45}, page_content=\"What is the capital of France?: What are the rules of basketball?\\nThe capital of France is Paris.: Two teams of five players aim to score points by shooting a ball into the opponent's hoop.\"), Document(metadata={'source': './data/FAQs.csv', 'row': 46}, page_content=\"What is the capital of France?: What are the top movies of all time?\\nThe capital of France is Paris.: 'The Godfather,' 'Titanic,' 'Inception,' 'Avengers: Endgame.'\"), Document(metadata={'source': './data/FAQs.csv', 'row': 47}, page_content='What is the capital of France?: Who are the most famous musicians?\\nThe capital of France is Paris.: Michael Jackson, The Beatles, Beyoncé, Elvis Presley.'), Document(metadata={'source': './data/FAQs.csv', 'row': 48}, page_content='What is the capital of France?: How can I stay motivated?\\nThe capital of France is Paris.: Set clear goals, stay positive, and track progress.'), Document(metadata={'source': './data/FAQs.csv', 'row': 49}, page_content='What is the capital of France?: What are some good daily habits?\\nThe capital of France is Paris.: Exercise, reading, meditation, planning the day.'), Document(metadata={'source': './data/FAQs.csv', 'row': 50}, page_content='What is the capital of France?: How do I improve time management?\\nThe capital of France is Paris.: Prioritize tasks, use planners, and avoid procrastination.'), Document(metadata={'source': './data/FAQs.csv', 'row': 51}, page_content='What is the capital of France?: What is photosynthesis?\\nThe capital of France is Paris.: The process by which green plants convert sunlight into energy.'), Document(metadata={'source': './data/FAQs.csv', 'row': 52}, page_content='What is the capital of France?: What is Newton’s first law of motion?\\nThe capital of France is Paris.: An object in motion stays in motion unless acted upon by an external force.'), Document(metadata={'source': './data/FAQs.csv', 'row': 53}, page_content='What is the capital of France?: What is the Pythagorean theorem?\\nThe capital of France is Paris.: A mathematical principle stating a² + b² = c² for right-angled triangles.'), Document(metadata={'source': './data/FAQs.csv', 'row': 54}, page_content='What is the capital of France?: What is the human respiratory system?\\nThe capital of France is Paris.: The system responsible for breathing and gas exchange in the body.'), Document(metadata={'source': './data/FAQs.csv', 'row': 55}, page_content='What is the capital of France?: What is a black hole?\\nThe capital of France is Paris.: A region of space with gravitational pull so strong that nothing can escape it.'), Document(metadata={'source': './data/FAQs.csv', 'row': 56}, page_content='What is the capital of France?: Who was Albert Einstein?\\nThe capital of France is Paris.: A physicist known for developing the theory of relativity.'), Document(metadata={'source': './data/FAQs.csv', 'row': 57}, page_content='What is the capital of France?: What is the boiling point of water?\\nThe capital of France is Paris.: 100°C or 212°F at sea level.'), Document(metadata={'source': './data/FAQs.csv', 'row': 58}, page_content='What is the capital of France?: How does an electric circuit work?\\nThe capital of France is Paris.: It allows electricity to flow through a connected path.'), Document(metadata={'source': './data/FAQs.csv', 'row': 59}, page_content=\"What is the capital of France?: What is a solar eclipse?\\nThe capital of France is Paris.: An event where the moon blocks the sun's light from reaching Earth.\"), Document(metadata={'source': './data/FAQs.csv', 'row': 60}, page_content='What is the capital of France?: What is the function of the liver?\\nThe capital of France is Paris.: It detoxifies blood, produces bile, and stores energy.'), Document(metadata={'source': './data/FAQs.csv', 'row': 61}, page_content='What is the capital of France?: What is the smallest unit of life?\\nThe capital of France is Paris.: The cell.'), Document(metadata={'source': './data/FAQs.csv', 'row': 62}, page_content='What is the capital of France?: What are atoms made of?\\nThe capital of France is Paris.: Protons, neutrons, and electrons.'), Document(metadata={'source': './data/FAQs.csv', 'row': 63}, page_content='What is the capital of France?: What is the law of conservation of energy?\\nThe capital of France is Paris.: Energy cannot be created or destroyed, only transformed.'), Document(metadata={'source': './data/FAQs.csv', 'row': 64}, page_content='What is the capital of France?: What are the four main blood types?\\nThe capital of France is Paris.: A, B, AB, and O.'), Document(metadata={'source': './data/FAQs.csv', 'row': 65}, page_content='What is the capital of France?: What is the periodic table?\\nThe capital of France is Paris.: A table of chemical elements organized by atomic number.'), Document(metadata={'source': './data/FAQs.csv', 'row': 66}, page_content='What is the capital of France?: What is quantum mechanics?\\nThe capital of France is Paris.: A branch of physics that studies particles at the atomic level.'), Document(metadata={'source': './data/FAQs.csv', 'row': 67}, page_content='What is the capital of France?: What is the role of mitochondria?\\nThe capital of France is Paris.: They generate energy for the cell, known as the powerhouse of the cell.'), Document(metadata={'source': './data/FAQs.csv', 'row': 68}, page_content='What is the capital of France?: What is the speed of sound?\\nThe capital of France is Paris.: Approximately 343 meters per second in air.'), Document(metadata={'source': './data/FAQs.csv', 'row': 69}, page_content='What is the capital of France?: What is an ecosystem?\\nThe capital of France is Paris.: A community of living organisms interacting with their environment.'), Document(metadata={'source': './data/FAQs.csv', 'row': 70}, page_content='What is the capital of France?: What is genetics?\\nThe capital of France is Paris.: The study of heredity and variation in organisms.'), Document(metadata={'source': './data/FAQs.csv', 'row': 71}, page_content='What is the capital of France?: What is the tallest mountain in the world?\\nThe capital of France is Paris.: Mount Everest is the tallest mountain in the world.'), Document(metadata={'source': './data/FAQs.csv', 'row': 72}, page_content='What is the capital of France?: What is the largest ocean on Earth?\\nThe capital of France is Paris.: The Pacific Ocean is the largest ocean on Earth.'), Document(metadata={'source': './data/FAQs.csv', 'row': 73}, page_content='What is the capital of France?: What is the main ingredient in guacamole?\\nThe capital of France is Paris.: Avocado is the main ingredient in guacamole.'), Document(metadata={'source': './data/FAQs.csv', 'row': 74}, page_content='What is the capital of France?: What is the currency of Japan?\\nThe capital of France is Paris.: The currency of Japan is the yen.'), Document(metadata={'source': './data/FAQs.csv', 'row': 75}, page_content='What is the capital of France?: What is the primary language spoken in Brazil?\\nThe capital of France is Paris.: The primary language spoken in Brazil is Portuguese.'), Document(metadata={'source': './data/FAQs.csv', 'row': 76}, page_content='What is the capital of France?: What is the fastest land animal?\\nThe capital of France is Paris.: The cheetah is the fastest land animal.'), Document(metadata={'source': './data/FAQs.csv', 'row': 77}, page_content='What is the capital of France?: What is the smallest country in the world?\\nThe capital of France is Paris.: Vatican City is the smallest country in the world.'), Document(metadata={'source': './data/FAQs.csv', 'row': 78}, page_content='What is the capital of France?: What is the main function of the heart?\\nThe capital of France is Paris.: The main function of the heart is to pump blood throughout the body.'), Document(metadata={'source': './data/FAQs.csv', 'row': 79}, page_content='What is the capital of France?: What is the largest mammal in the world?\\nThe capital of France is Paris.: The blue whale is the largest mammal in the world.'), Document(metadata={'source': './data/FAQs.csv', 'row': 80}, page_content='What is the capital of France?: What is the primary function of the kidneys?\\nThe capital of France is Paris.: The primary function of the kidneys is to filter waste from the blood.'), Document(metadata={'source': './data/FAQs.csv', 'row': 81}, page_content='What is the capital of France?: What is the capital of Australia?\\nThe capital of France is Paris.: The capital of Australia is Canberra.'), Document(metadata={'source': './data/FAQs.csv', 'row': 82}, page_content='What is the capital of France?: What is the most spoken language in the world?\\nThe capital of France is Paris.: Mandarin Chinese is the most spoken language in the world.'), Document(metadata={'source': './data/FAQs.csv', 'row': 83}, page_content='What is the capital of France?: What is the primary purpose of photosynthesis?\\nThe capital of France is Paris.: The primary purpose of photosynthesis is to convert sunlight into energy for plants.'), Document(metadata={'source': './data/FAQs.csv', 'row': 84}, page_content=\"What is the capital of France?: What is the main component of the Earth's atmosphere?\\nThe capital of France is Paris.: Nitrogen is the main component of the Earth's atmosphere.\"), Document(metadata={'source': './data/FAQs.csv', 'row': 85}, page_content='What is the capital of France?: What is the largest desert in the world?\\nThe capital of France is Paris.: The Antarctic Desert is the largest desert in the world.'), Document(metadata={'source': './data/FAQs.csv', 'row': 86}, page_content='What is the capital of France?: What is the boiling point of water at sea level?\\nThe capital of France is Paris.: The boiling point of water at sea level is 100°C or 212°F.'), Document(metadata={'source': './data/FAQs.csv', 'row': 87}, page_content='What is the capital of France?: What is the primary function of the lungs?\\nThe capital of France is Paris.: The primary function of the lungs is to facilitate gas exchange.'), Document(metadata={'source': './data/FAQs.csv', 'row': 88}, page_content='What is the capital of France?: What is the capital of Canada?\\nThe capital of France is Paris.: The capital of Canada is Ottawa.'), Document(metadata={'source': './data/FAQs.csv', 'row': 89}, page_content=\"What is the capital of France?: What is the most abundant gas in the Earth's atmosphere?\\nThe capital of France is Paris.: Nitrogen is the most abundant gas in the Earth's atmosphere.\"), Document(metadata={'source': './data/FAQs.csv', 'row': 90}, page_content='What is the capital of France?: What is the primary purpose of the digestive system?\\nThe capital of France is Paris.: The primary purpose of the digestive system is to break down food and absorb nutrients.'), Document(metadata={'source': './data/FAQs.csv', 'row': 91}, page_content='What is the capital of France?: What is the largest organ in the human body?\\nThe capital of France is Paris.: The skin is the largest organ in the human body.'), Document(metadata={'source': './data/FAQs.csv', 'row': 92}, page_content='What is the capital of France?: What is the main ingredient in bread?\\nThe capital of France is Paris.: Flour is the main ingredient in bread.'), Document(metadata={'source': './data/FAQs.csv', 'row': 93}, page_content='What is the capital of France?: What is the capital of Italy?\\nThe capital of France is Paris.: The capital of Italy is Rome.'), Document(metadata={'source': './data/FAQs.csv', 'row': 94}, page_content='What is the capital of France?: What is the primary function of the brain?\\nThe capital of France is Paris.: The primary function of the brain is to process information and control bodily functions.'), Document(metadata={'source': './data/FAQs.csv', 'row': 95}, page_content='What is the capital of France?: What is the most widely practiced religion in the world?\\nThe capital of France is Paris.: Christianity is the most widely practiced religion in the world.'), Document(metadata={'source': './data/FAQs.csv', 'row': 96}, page_content='What is the capital of France?: What is the capital of Germany?\\nThe capital of France is Paris.: The capital of Germany is Berlin.'), Document(metadata={'source': './data/FAQs.csv', 'row': 97}, page_content='What is the capital of France?: What is the primary purpose of the skeletal system?\\nThe capital of France is Paris.: The primary purpose of the skeletal system is to provide structure and support to the body.'), Document(metadata={'source': './data/FAQs.csv', 'row': 98}, page_content='What is the capital of France?: What is the largest planet in our solar system?\\nThe capital of France is Paris.: Jupiter is the largest planet in our solar system.'), Document(metadata={'source': './data/FAQs.csv', 'row': 99}, page_content='What is the capital of France?: What is the capital of Spain?\\nThe capital of France is Paris.: The capital of Spain is Madrid.'), Document(metadata={'source': './data/FAQs.csv', 'row': 100}, page_content='What is the capital of France?: What is the primary function of the liver?\\nThe capital of France is Paris.: The primary function of the liver is to detoxify chemicals and metabolize drugs.'), Document(metadata={'source': './data/FAQs.csv', 'row': 101}, page_content='What is the capital of France?: What is the capital of Russia?\\nThe capital of France is Paris.: The capital of Russia is Moscow.'), Document(metadata={'source': './data/FAQs.csv', 'row': 102}, page_content='What is the capital of France?: What is the primary purpose of the immune system?\\nThe capital of France is Paris.: The primary purpose of the immune system is to defend the body against infections.'), Document(metadata={'source': './data/FAQs.csv', 'row': 103}, page_content='What is the capital of France?: What is the capital of China?\\nThe capital of France is Paris.: The capital of China is Beijing.'), Document(metadata={'source': './data/FAQs.csv', 'row': 104}, page_content='What is the capital of France?: What is the primary function of the pancreas?\\nThe capital of France is Paris.: The primary function of the pancreas is to produce insulin and digestive enzymes.'), Document(metadata={'source': './data/FAQs.csv', 'row': 105}, page_content='What is the capital of France?: What is the capital of India?\\nThe capital of France is Paris.: The capital of India is New Delhi.'), Document(metadata={'source': './data/FAQs.csv', 'row': 106}, page_content='What is the capital of France?: What is the primary purpose of the circulatory system?\\nThe capital of France is Paris.: The primary purpose of the circulatory system is to transport blood and nutrients throughout the body.'), Document(metadata={'source': './data/FAQs.csv', 'row': 107}, page_content='What is the capital of France?: What is the capital of Egypt?\\nThe capital of France is Paris.: The capital of Egypt is Cairo.'), Document(metadata={'source': './data/FAQs.csv', 'row': 108}, page_content='What is the capital of France?: What is the primary function of the nervous system?\\nThe capital of France is Paris.: The primary function of the nervous system is to transmit signals between different parts of the body.'), Document(metadata={'source': './data/FAQs.csv', 'row': 109}, page_content='What is the capital of France?: What is the capital of Mexico?\\nThe capital of France is Paris.: The capital of Mexico is Mexico City.'), Document(metadata={'source': './data/FAQs.csv', 'row': 110}, page_content='What is the capital of France?: What is the primary purpose of the reproductive system?\\nThe capital of France is Paris.: The primary purpose of the reproductive system is to produce offspring.'), Document(metadata={'source': './data/FAQs.csv', 'row': 111}, page_content='What is the capital of France?: What is the capital of France?\\nThe capital of France is Paris.: The capital of France is Paris.'), Document(metadata={'source': './data/FAQs.csv', 'row': 112}, page_content='What is the capital of France?: What is the primary function of the thyroid gland?\\nThe capital of France is Paris.: The primary function of the thyroid gland is to regulate metabolism.'), Document(metadata={'source': './data/FAQs.csv', 'row': 113}, page_content='What is the capital of France?: What is the capital of the United Kingdom?\\nThe capital of France is Paris.: The capital of the United Kingdom is London.'), Document(metadata={'source': './data/FAQs.csv', 'row': 114}, page_content='What is the capital of France?: What is the primary purpose of the muscular system?\\nThe capital of France is Paris.: The primary purpose of the muscular system is to facilitate movement.'), Document(metadata={'source': './data/FAQs.csv', 'row': 115}, page_content='What is the capital of France?: What is the capital of Brazil?\\nThe capital of France is Paris.: The capital of Brazil is Brasília.'), Document(metadata={'source': './data/FAQs.csv', 'row': 116}, page_content='What is the capital of France?: What is the primary function of the spleen?\\nThe capital of France is Paris.: The primary function of the spleen is to filter blood and support the immune system.'), Document(metadata={'source': './data/FAQs.csv', 'row': 117}, page_content='What is the capital of France?: What is the capital of Argentina?\\nThe capital of France is Paris.: The capital of Argentina is Buenos Aires.'), Document(metadata={'source': './data/FAQs.csv', 'row': 118}, page_content='What is the capital of France?: What is the primary purpose of the integumentary system?\\nThe capital of France is Paris.: The primary purpose of the integ umentary system is to protect the body from external damage and regulate temperature.'), Document(metadata={'source': './data/FAQs.csv', 'row': 119}, page_content='What is the capital of France?: What is the capital of South Africa?\\nThe capital of France is Paris.: The capital of South Africa is Pretoria (administrative), Bloemfontein (judicial), and Cape Town (legislative).'), Document(metadata={'source': './data/FAQs.csv', 'row': 120}, page_content='What is the capital of France?: What is the primary function of the gallbladder?\\nThe capital of France is Paris.: The primary function of the gallbladder is to store and concentrate bile.'), Document(metadata={'source': './data/FAQs.csv', 'row': 121}, page_content='What is the capital of France?: What is the capital of Turkey?\\nThe capital of France is Paris.: The capital of Turkey is Ankara.'), Document(metadata={'source': './data/FAQs.csv', 'row': 122}, page_content='What is the capital of France?: What is the primary purpose of the endocrine system?\\nThe capital of France is Paris.: The primary purpose of the endocrine system is to regulate bodily functions through hormones.'), Document(metadata={'source': './data/FAQs.csv', 'row': 123}, page_content='What is the capital of France?: What is the capital of Thailand?\\nThe capital of France is Paris.: The capital of Thailand is Bangkok.'), Document(metadata={'source': './data/FAQs.csv', 'row': 124}, page_content='What is the capital of France?: What is the primary function of the small intestine?\\nThe capital of France is Paris.: The primary function of the small intestine is to absorb nutrients from food.'), Document(metadata={'source': './data/FAQs.csv', 'row': 125}, page_content='What is the capital of France?: What is the capital of Indonesia?\\nThe capital of France is Paris.: The capital of Indonesia is Jakarta.'), Document(metadata={'source': './data/FAQs.csv', 'row': 126}, page_content='What is the capital of France?: What is the primary purpose of the large intestine?\\nThe capital of France is Paris.: The primary purpose of the large intestine is to absorb water and electrolytes and form waste.'), Document(metadata={'source': './data/FAQs.csv', 'row': 127}, page_content='What is the capital of France?: What is the capital of Nigeria?\\nThe capital of France is Paris.: The capital of Nigeria is Abuja.'), Document(metadata={'source': './data/FAQs.csv', 'row': 128}, page_content='What is the capital of France?: What is the primary function of the adrenal glands?\\nThe capital of France is Paris.: The primary function of the adrenal glands is to produce hormones that help regulate metabolism, immune response, and stress.'), Document(metadata={'source': './data/FAQs.csv', 'row': 129}, page_content='What is the capital of France?: What is the capital of Vietnam?\\nThe capital of France is Paris.: The capital of Vietnam is Hanoi.'), Document(metadata={'source': './data/FAQs.csv', 'row': 130}, page_content='What is the capital of France?: What is the primary purpose of the urinary system?\\nThe capital of France is Paris.: The primary purpose of the urinary system is to remove waste and excess fluids from the body.'), Document(metadata={'source': './data/FAQs.csv', 'row': 131}, page_content='What is the capital of France?: What is the capital of Saudi Arabia?\\nThe capital of France is Paris.: The capital of Saudi Arabia is Riyadh.'), Document(metadata={'source': './data/FAQs.csv', 'row': 132}, page_content='What is the capital of France?: What is the primary function of the skin?\\nThe capital of France is Paris.: The primary function of the skin is to act as a barrier and protect the body from external harm.'), Document(metadata={'source': './data/FAQs.csv', 'row': 133}, page_content='What is the capital of France?: What is the capital of Sweden?\\nThe capital of France is Paris.: The capital of Sweden is Stockholm.'), Document(metadata={'source': './data/FAQs.csv', 'row': 134}, page_content='What is the capital of France?: What is the primary purpose of the lymphatic system?\\nThe capital of France is Paris.: The primary purpose of the lymphatic system is to help defend the body against infections and maintain fluid balance.'), Document(metadata={'source': './data/FAQs.csv', 'row': 135}, page_content='What is the capital of France?: What is the capital of Norway?\\nThe capital of France is Paris.: The capital of Norway is Oslo.'), Document(metadata={'source': './data/FAQs.csv', 'row': 136}, page_content='What is the capital of France?: What is the primary function of the esophagus?\\nThe capital of France is Paris.: The primary function of the esophagus is to transport food from the mouth to the stomach.'), Document(metadata={'source': './data/FAQs.csv', 'row': 137}, page_content='What is the capital of France?: What is the capital of Finland?\\nThe capital of France is Paris.: The capital of Finland is Helsinki.'), Document(metadata={'source': './data/FAQs.csv', 'row': 138}, page_content='What is the capital of France?: What is the primary purpose of the respiratory system?\\nThe capital of France is Paris.: The primary purpose of the respiratory system is to facilitate breathing and gas exchange.'), Document(metadata={'source': './data/FAQs.csv', 'row': 139}, page_content='What is the capital of France?: What is the capital of Denmark?\\nThe capital of France is Paris.: The capital of Denmark is Copenhagen.'), Document(metadata={'source': './data/FAQs.csv', 'row': 140}, page_content='What is the capital of France?: What is the primary function of the stomach?\\nThe capital of France is Paris.: The primary function of the stomach is to break down food using acids and enzymes.'), Document(metadata={'source': './data/FAQs.csv', 'row': 141}, page_content='What is the capital of France?: What is the capital of Greece?\\nThe capital of France is Paris.: The capital of Greece is Athens.'), Document(metadata={'source': './data/FAQs.csv', 'row': 142}, page_content='What is the capital of France?: What is the primary purpose of the reproductive system in males?\\nThe capital of France is Paris.: The primary purpose of the reproductive system in males is to produce sperm and hormones.'), Document(metadata={'source': './data/FAQs.csv', 'row': 143}, page_content='What is the capital of France?: What is the capital of Portugal?\\nThe capital of France is Paris.: The capital of Portugal is Lisbon.'), Document(metadata={'source': './data/FAQs.csv', 'row': 144}, page_content='What is the capital of France?: What is the primary purpose of the reproductive system in females?\\nThe capital of France is Paris.: The primary purpose of the reproductive system in females is to produce eggs and support fetal development.'), Document(metadata={'source': './data/FAQs.csv', 'row': 145}, page_content='What is the capital of France?: What is the capital of Hungary?\\nThe capital of France is Paris.: The capital of Hungary is Budapest.'), Document(metadata={'source': './data/FAQs.csv', 'row': 146}, page_content='What is the capital of France?: What is the primary function of the cerebellum?\\nThe capital of France is Paris.: The primary function of the cerebellum is to coordinate voluntary movements and maintain balance.'), Document(metadata={'source': './data/FAQs.csv', 'row': 147}, page_content='What is the capital of France?: What is the capital of Czech Republic?\\nThe capital of France is Paris.: The capital of Czech Republic is Prague.'), Document(metadata={'source': './data/FAQs.csv', 'row': 148}, page_content='What is the capital of France?: What is the primary purpose of the occipital lobe?\\nThe capital of France is Paris.: The primary purpose of the occipital lobe is to process visual information.'), Document(metadata={'source': './data/FAQs.csv', 'row': 149}, page_content='What is the capital of France?: What is the capital of Romania?\\nThe capital of France is Paris.: The capital of Romania is Bucharest.'), Document(metadata={'source': './data/FAQs.csv', 'row': 150}, page_content='What is the capital of France?: What is the primary function of the temporal lobe?\\nThe capital of France is Paris.: The primary function of the temporal lobe is to process auditory information and is involved in memory.'), Document(metadata={'source': './data/FAQs.csv', 'row': 151}, page_content='What is the capital of France?: What is the capital of Bulgaria?\\nThe capital of France is Paris.: The capital of Bulgaria is Sofia.'), Document(metadata={'source': './data/FAQs.csv', 'row': 152}, page_content='What is the capital of France?: What is the primary purpose of the frontal lobe?\\nThe capital of France is Paris.: The primary purpose of the frontal lobe is to control reasoning, planning, and problem-solving.'), Document(metadata={'source': './data/FAQs.csv', 'row': 153}, page_content='What is the capital of France?: What is the capital of Slovakia?\\nThe capital of France is Paris.: The capital of Slovakia is Bratislava.'), Document(metadata={'source': './data/FAQs.csv', 'row': 154}, page_content='What is the capital of France?: What is the primary function of the parietal lobe?\\nThe capital of France is Paris.: The primary function of the parietal lobe is to process sensory information and spatial awareness.'), Document(metadata={'source': './data/FAQs.csv', 'row': 155}, page_content='What is the capital of France?: What is the capital of Serbia?\\nThe capital of France is Paris.: The capital of Serbia is Belgrade.'), Document(metadata={'source': './data/FAQs.csv', 'row': 156}, page_content='What is the capital of France?: What is the primary purpose of the corpus callosum?\\nThe capital of France is Paris.: The primary purpose of the corpus callosum is to connect the left and right hemispheres of the brain.'), Document(metadata={'source': './data/FAQs.csv', 'row': 157}, page_content='What is the capital of France?: What is the capital of Croatia?\\nThe capital of France is Paris.: The capital of Croatia is Zagreb.'), Document(metadata={'source': './data/FAQs.csv', 'row': 158}, page_content='What is the capital of France?: What is the primary function of the hypothalamus?\\nThe capital of France is Paris.: The primary function of the hypothalamus is to regulate body temperature, hunger, and thirst.'), Document(metadata={'source': './data/FAQs.csv', 'row': 159}, page_content='What is the capital of France?: What is the capital of Slovenia?\\nThe capital of France is Paris.: The capital of Slovenia is Ljubljana.'), Document(metadata={'source': './data/FAQs.csv', 'row': 160}, page_content='What is the capital of France?: What is the primary purpose of the thalamus?\\nThe capital of France is Paris.: The primary purpose of the thalamus is to relay sensory information to the appropriate areas of the brain.'), Document(metadata={'source': './data/FAQs.csv', 'row': 161}, page_content='What is the capital of France?: What is the capital of Malta?\\nThe capital of France is Paris.: The capital of Malta is Valletta.'), Document(metadata={'source': './data/FAQs.csv', 'row': 162}, page_content='What is the capital of France?: What is the primary function of the amygdala?\\nThe capital of France is Paris.: The primary function of the amygdala is to')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "csv_loader = CSVLoader(file_path=\"./data/FAQs.csv\")\n",
    "\n",
    "csv_data = csv_loader.load()\n",
    "\n",
    "print(csv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'C:\\\\Users\\\\adeel\\\\Langchain\\\\Retrival_From_Multiple_files\\\\data\\\\FAQs.json', 'seq_num': 1}, page_content='[{\"question\": \"What is the capital of France?\", \"answer\": \"The capital of France is Paris.\"}, {\"question\": \"How many continents are there in the world?\", \"answer\": \"There are seven continents: Africa, Antarctica, Asia, Europe, North America, Oceania, and South America.\"}, {\"question\": \"What is the largest planet in our solar system?\", \"answer\": \"Jupiter is the largest planet in our solar system.\"}, {\"question\": \"Who discovered gravity?\", \"answer\": \"Sir Isaac Newton discovered gravity.\"}, {\"question\": \"What are the three states of matter?\", \"answer\": \"Solid, liquid, and gas.\"}, {\"question\": \"Who wrote \\'Romeo and Juliet\\'?\", \"answer\": \"William Shakespeare.\"}, {\"question\": \"What is the chemical symbol for water?\", \"answer\": \"H2O.\"}, {\"question\": \"What is the speed of light?\", \"answer\": \"Approximately 299,792,458 meters per second (about 300,000 km/s).\"}, {\"question\": \"Who was the first person to walk on the moon?\", \"answer\": \"Neil Armstrong in 1969.\"}, {\"question\": \"What is artificial intelligence (AI)?\", \"answer\": \"AI is the simulation of human intelligence in machines that can think, learn, and solve problems.\"}, {\"question\": \"What does CPU stand for?\", \"answer\": \"Central Processing Unit.\"}, {\"question\": \"What is the difference between RAM and ROM?\", \"answer\": \"RAM is temporary memory used for active tasks, while ROM is permanent storage used for system instructions.\"}, {\"question\": \"What is an IP address?\", \"answer\": \"It is a unique numerical address assigned to a device on a network.\"}, {\"question\": \"What does HTTP stand for?\", \"answer\": \"Hypertext Transfer Protocol.\"}, {\"question\": \"What is cloud computing?\", \"answer\": \"Cloud computing is the delivery of computing services over the internet instead of local servers.\"}, {\"question\": \"What is an operating system?\", \"answer\": \"It is software that manages hardware and software resources on a computer (e.g., Windows, macOS, Linux).\"}, {\"question\": \"What does HTML stand for?\", \"answer\": \"HyperText Markup Language.\"}, {\"question\": \"What is the difference between a website and a web application?\", \"answer\": \"A website is informational, while a web application is interactive and allows user input.\"}, {\"question\": \"What is cybersecurity?\", \"answer\": \"The practice of protecting systems, networks, and data from cyber threats.\"}, {\"question\": \"What is DNA?\", \"answer\": \"DNA (Deoxyribonucleic Acid) carries genetic information in living organisms.\"}, {\"question\": \"What causes seasons on Earth?\", \"answer\": \"The tilt of the Earth\\'s axis as it orbits the sun.\"}, {\"question\": \"How does the human brain function?\", \"answer\": \"It processes information, controls the body, and enables thinking and emotions.\"}, {\"question\": \"What is the immune system?\", \"answer\": \"The body\\'s defense system against infections and diseases.\"}, {\"question\": \"What are the benefits of exercise?\", \"answer\": \"Improves heart health, strengthens muscles, boosts mood, and enhances overall well-being.\"}, {\"question\": \"How does the heart work?\", \"answer\": \"It pumps oxygen-rich blood throughout the body and removes carbon dioxide.\"}, {\"question\": \"What are renewable and nonrenewable energy sources?\", \"answer\": \"Renewable: Solar, wind, hydro. Nonrenewable: Coal, oil, natural gas.\"}, {\"question\": \"What is the greenhouse effect?\", \"answer\": \"The trapping of heat in Earth\\\\u2019s atmosphere by greenhouse gases.\"}, {\"question\": \"What is a virus?\", \"answer\": \"A microscopic infectious agent that replicates inside a host cell.\"}, {\"question\": \"Why do we need sleep?\", \"answer\": \"For brain function, body repair, and overall health.\"}, {\"question\": \"What is inflation?\", \"answer\": \"The increase in prices over time, reducing purchasing power.\"}, {\"question\": \"How do credit cards work?\", \"answer\": \"They allow you to borrow money from a bank for purchases and repay later, often with interest.\"}, {\"question\": \"What is cryptocurrency?\", \"answer\": \"A digital currency that uses encryption for security (e.g., Bitcoin).\"}, {\"question\": \"What is a stock market?\", \"answer\": \"A marketplace where stocks are bought and sold.\"}, {\"question\": \"What is the difference between debit and credit?\", \"answer\": \"Debit withdraws money you own; credit borrows money.\"}, {\"question\": \"How do bank loans work?\", \"answer\": \"A bank lends you money that must be repaid with interest.\"}, {\"question\": \"What is an investment?\", \"answer\": \"Allocating money to an asset to gain profit.\"}, {\"question\": \"What is compound interest?\", \"answer\": \"Interest earned on both the initial principal and accumulated interest.\"}, {\"question\": \"What does GDP stand for?\", \"answer\": \"Gross Domestic Product, the total value of goods and services in a country.\"}, {\"question\": \"What is entrepreneurship?\", \"answer\": \"Starting and managing a business venture.\"}, {\"question\": \"What are the top tourist destinations in the world?\", \"answer\": \"Paris, New York, London, Dubai, Tokyo.\"}, {\"question\": \"What are the safest countries to travel to?\", \"answer\": \"Switzerland, Canada, Denmark, Iceland.\"}, {\"question\": \"How do I apply for a passport?\", \"answer\": \"Submit an application with required documents to your country\\'s passport office.\"}, {\"question\": \"What is a visa?\", \"answer\": \"A permit that allows you to enter a foreign country.\"}, {\"question\": \"How can I find cheap flights?\", \"answer\": \"Use fare comparison websites, book early, and be flexible with dates.\"}, {\"question\": \"What are the most popular sports in the world?\", \"answer\": \"Soccer, basketball, cricket, tennis, baseball.\"}, {\"question\": \"What is the FIFA World Cup?\", \"answer\": \"The biggest international soccer tournament, held every four years.\"}, {\"question\": \"What are the rules of basketball?\", \"answer\": \"Two teams of five players aim to score points by shooting a ball into the opponent\\'s hoop.\"}, {\"question\": \"What are the top movies of all time?\", \"answer\": \"\\'The Godfather,\\' \\'Titanic,\\' \\'Inception,\\' \\'Avengers: Endgame.\\'\"}, {\"question\": \"Who are the most famous musicians?\", \"answer\": \"Michael Jackson, The Beatles, Beyonc\\\\u00e9, Elvis Presley.\"}, {\"question\": \"How can I stay motivated?\", \"answer\": \"Set clear goals, stay positive, and track progress.\"}, {\"question\": \"What are some good daily habits?\", \"answer\": \"Exercise, reading, meditation, planning the day.\"}, {\"question\": \"How do I improve time management?\", \"answer\": \"Prioritize tasks, use planners, and avoid procrastination.\"}, {\"question\": \"What is photosynthesis?\", \"answer\": \"The process by which green plants convert sunlight into energy.\"}, {\"question\": \"What is Newton\\\\u2019s first law of motion?\", \"answer\": \"An object in motion stays in motion unless acted upon by an external force.\"}, {\"question\": \"What is the Pythagorean theorem?\", \"answer\": \"A mathematical principle stating a\\\\u00b2 + b\\\\u00b2 = c\\\\u00b2 for right-angled triangles.\"}, {\"question\": \"What is the human respiratory system?\", \"answer\": \"The system responsible for breathing and gas exchange in the body.\"}, {\"question\": \"What is a black hole?\", \"answer\": \"A region of space with gravitational pull so strong that nothing can escape it.\"}, {\"question\": \"Who was Albert Einstein?\", \"answer\": \"A physicist known for developing the theory of relativity.\"}, {\"question\": \"What is the boiling point of water?\", \"answer\": \"100\\\\u00b0C or 212\\\\u00b0F at sea level.\"}, {\"question\": \"How does an electric circuit work?\", \"answer\": \"It allows electricity to flow through a connected path.\"}, {\"question\": \"What is a solar eclipse?\", \"answer\": \"An event where the moon blocks the sun\\'s light from reaching Earth.\"}, {\"question\": \"What is the function of the liver?\", \"answer\": \"It detoxifies blood, produces bile, and stores energy.\"}, {\"question\": \"What is the smallest unit of life?\", \"answer\": \"The cell.\"}, {\"question\": \"What are atoms made of?\", \"answer\": \"Protons, neutrons, and electrons.\"}, {\"question\": \"What is the law of conservation of energy?\", \"answer\": \"Energy cannot be created or destroyed, only transformed.\"}, {\"question\": \"What are the four main blood types?\", \"answer\": \"A, B, AB, and O.\"}, {\"question\": \"What is the periodic table?\", \"answer\": \"A table of chemical elements organized by atomic number.\"}, {\"question\": \"What is quantum mechanics?\", \"answer\": \"A branch of physics that studies particles at the atomic level.\"}, {\"question\": \"What is the role of mitochondria?\", \"answer\": \"They generate energy for the cell, known as the powerhouse of the cell.\"}, {\"question\": \"What is the speed of sound?\", \"answer\": \"Approximately 343 meters per second in air.\"}, {\"question\": \"What is an ecosystem?\", \"answer\": \"A community of living organisms interacting with their environment.\"}, {\"question\": \"What is genetics?\", \"answer\": \"The study of heredity and variation in organisms.\"}, {\"question\": \"What is the tallest mountain in the world?\", \"answer\": \"Mount Everest is the tallest mountain in the world.\"}, {\"question\": \"What is the largest ocean on Earth?\", \"answer\": \"The Pacific Ocean is the largest ocean on Earth.\"}, {\"question\": \"What is the main ingredient in guacamole?\", \"answer\": \"Avocado is the main ingredient in guacamole.\"}, {\"question\": \"What is the currency of Japan?\", \"answer\": \"The currency of Japan is the yen.\"}, {\"question\": \"What is the primary language spoken in Brazil?\", \"answer\": \"The primary language spoken in Brazil is Portuguese.\"}, {\"question\": \"What is the fastest land animal?\", \"answer\": \"The cheetah is the fastest land animal.\"}, {\"question\": \"What is the smallest country in the world?\", \"answer\": \"Vatican City is the smallest country in the world.\"}, {\"question\": \"What is the main function of the heart?\", \"answer\": \"The main function of the heart is to pump blood throughout the body.\"}, {\"question\": \"What is the largest mammal in the world?\", \"answer\": \"The blue whale is the largest mammal in the world.\"}, {\"question\": \"What is the primary function of the kidneys?\", \"answer\": \"The primary function of the kidneys is to filter waste from the blood.\"}, {\"question\": \"What is the capital of Australia?\", \"answer\": \"The capital of Australia is Canberra.\"}, {\"question\": \"What is the most spoken language in the world?\", \"answer\": \"Mandarin Chinese is the most spoken language in the world.\"}, {\"question\": \"What is the primary purpose of photosynthesis?\", \"answer\": \"The primary purpose of photosynthesis is to convert sunlight into energy for plants.\"}, {\"question\": \"What is the main component of the Earth\\'s atmosphere?\", \"answer\": \"Nitrogen is the main component of the Earth\\'s atmosphere.\"}, {\"question\": \"What is the largest desert in the world?\", \"answer\": \"The Antarctic Desert is the largest desert in the world.\"}, {\"question\": \"What is the boiling point of water at sea level?\", \"answer\": \"The boiling point of water at sea level is 100\\\\u00b0C or 212\\\\u00b0F.\"}, {\"question\": \"What is the primary function of the lungs?\", \"answer\": \"The primary function of the lungs is to facilitate gas exchange.\"}, {\"question\": \"What is the capital of Canada?\", \"answer\": \"The capital of Canada is Ottawa.\"}, {\"question\": \"What is the most abundant gas in the Earth\\'s atmosphere?\", \"answer\": \"Nitrogen is the most abundant gas in the Earth\\'s atmosphere.\"}, {\"question\": \"What is the primary purpose of the digestive system?\", \"answer\": \"The primary purpose of the digestive system is to break down food and absorb nutrients.\"}, {\"question\": \"What is the largest organ in the human body?\", \"answer\": \"The skin is the largest organ in the human body.\"}, {\"question\": \"What is the main ingredient in bread?\", \"answer\": \"Flour is the main ingredient in bread.\"}, {\"question\": \"What is the capital of Italy?\", \"answer\": \"The capital of Italy is Rome.\"}]')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import JSONLoader\n",
    "\n",
    "# Path to your JSON file\n",
    "json_file_path = \"./data/FAQs.json\"\n",
    "\n",
    "# Correct jq_schema syntax\n",
    "json_loader = JSONLoader(\n",
    "    file_path=json_file_path,\n",
    "    jq_schema='map({question, answer})',  # Extracts both question and answer as key-value pairs\n",
    "    text_content=False  # Store them in metadata instead of page_content\n",
    ")\n",
    "\n",
    "# Load data\n",
    "json_data = json_loader.load()\n",
    "\n",
    "print(json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': './data/LLMS.pdf', 'page': 0}, page_content='A REVIEW ON LARGE LANGUAGE MODELS : A RCHITECTURES ,\\nAPPLICATIONS , TAXONOMIES , OPEN ISSUES AND CHALLENGES\\nMohaimenul Azam Khan Raiaan1, Md. Saddam Hossain Mukta1, Kaniz Fatema2, Nur Mohammad Fahad1,\\nSadman Sakib1, Most. Marufatul Jannat Mim1, Jubaer Ahmad1 Mohammed Eunus Ali3,and Sami Azam2\\n1 Department of CSE, United International University (UIU), Dhaka-1212, Bangladesh\\n2 Faculty Science and Technology, Charles Darwin University, Australia\\n3 Department of CSE, Bangladesh University of Engineering and Technology (BUET), Dhaka-1000, Bangladesh\\nEmail: mraiaan191228@bscse.uiu.ac.bd, saddam@cse.uiu.ac.bd, kaniz.fatema@cdu.edu.au, {nfahad191040,\\nssakib191097, mmim192004, jahmad181023}@bscse.uiu.ac.bd, eunus@cse.buet.ac.bd, Sami.Azam@cdu.edu.au\\nABSTRACT\\nLarge Language Models (LLMs) recently demonstrated extraordinary capability, including natural\\nlanguage processing (NLP), language translation, text generation, question answering, etc. Moreover,\\nLLMs are a new and essential part of computerized language processing, having the ability to\\nunderstand complex verbal patterns and generate coherent and appropriate replies for the situation.\\nThough this success of LLMs has prompted a substantial increase in research contributions, rapid\\ngrowth has made it difficult to understand the overall impact of these improvements. Since a lot of\\nnew research on LLMs is coming out quickly, it is getting tough to get an overview of all of them in a\\nshort note. Consequently, the research community would benefit from a short but thorough review\\nof the recent changes in this area. This article thoroughly overviews LLMs, including their history,\\narchitectures, transformers, resources, training methods, applications, impacts, challenges, etc. This\\npaper begins by discussing the fundamental concepts of LLMs with its traditional pipeline of the\\nLLM training phase. It then provides an overview of the existing works, the history of LLMs, their\\nevolution over time, the architecture of transformers in LLMs, the different resources of LLMs, and\\nthe different training methods that have been used to train them. It also demonstrated the datasets\\nutilized in the studies. After that, the paper discusses the wide range of applications of LLMs,\\nincluding biomedical and healthcare, education, social, business, and agriculture. It also illustrates\\nhow LLMs create an impact on society and shape the future of AI and how they can be used to\\nsolve real-world problems. Then it also explores open issues and challenges to deploying LLMs\\nin real-world aspects, including ethical issues, model biases, computing resources, interoperability,\\ncontextual constraints, privacy, security, etc. It also discusses methods to improve the robustness\\nand controllability of LLMs. Finally, the study analyses the future of LLM research and issues that\\nneed to be overcome to make LLMs more impactful and reliable. However, this review paper aims to\\nhelp practitioners, researchers, and experts thoroughly understand the evolution of LLMs, pre-trained\\narchitectures, applications, challenges, and future goals. Furthermore, it serves as a valuable reference\\nfor future development and application of LLM in numerous practical domains.\\nKeywords Large Language Models, Natural Language Processing, Evolution, Transformer, Pre-trained models,\\nTaxonomy, Application\\n1 Introduction\\nLanguage is a remarkable tool for human expression and communication, one that begins to emerge in infancy and\\nmakers throughout a lifetime [1, 2]. Nevertheless, machines are unable to possess the innate ability to understand and\\nspeak in human language without the help of sophisticated artificial intelligence (AI) [3]. Therefore, a long-standing\\nscientific challenge and aim has been to achieve human-like reading, writing, and communication skills in machines [4].\\nHowever, advances in deep learning approaches, the availability of immense computer resources, and the availability'), Document(metadata={'source': './data/LLMS.pdf', 'page': 1}, page_content='A Review on LLM: Architectures, Applications, Open Issues and Challenges\\nof vast quantities of training data all contributed to the emergence of large language models (LLMs). It is a category\\nof language models that utilizes neural networks containing billions of parameters, trained on enormous quantities\\nof unlabeled text data using a self-supervised learning approach [5]. It is considered a huge step forward in natural\\nlanguage processing (NLP) and AI [ 6]. These models, frequently pre-trained on large corpora from the web, may\\nlearn complicated patterns, language subtleties, and semantic linkages. Besides, they have proved their ability in\\nvarious language-related tasks, including text synthesis, translation, summarization, question-answering, and sentiment\\nanalysis, by leveraging deep learning techniques and large datasets. Moreover, the results of fine-tuning these models\\non specific downstream tasks have been quite promising, with state-of-the-art performance in several benchmarks [7].\\nLLMs have their roots in the early development of language models and neural networks. Statistical approaches and\\nn-gram models were used in earlier attempts to develop language models [8], but these models have shortcomings in\\nexpressing long-term interdependence and context in language. After that, researchers began to explore more complex\\nways with the development of neural networks and the availability of larger datasets. The creation of the Recurrent\\nNeural Network (RNN) [ 9], which allowed for the modeling of sequential data, including language, was a crucial\\nmilestone. However, RNNs were limited in their efficacy due to vanishing gradients and long-term dependencies. The\\nsignificant advancement in LLMs systems occurred when the transformer architecture was introduced in the seminal\\nwork [10]. The transformer model is built around the self-attention mechanism, enabling parallelization and efficient\\nhandling of long-range dependencies. Furthermore, it served as the basis for models such as Google’s Bidirectional\\nEncoder Representations from Transformers [ 11] and open AI’s Generative Pre-trained Transformer (GPT) series,\\nwhich excelled at various language tasks.\\nFigure 1: Pipeline of the LLM training phase\\nThe pipeline of the basic LLM architecture is shown in Figure 1. It receives text data from a variety of sources\\nand then forwards it to the subsequent stage for preprocessing. It then completes its training process by executing a\\nseries of stages, including random parameter initialization, numerical data input, loss function calculation, parameter\\noptimization, and iterative training. They offer text translation, text summarization, sentiment analysis, and other\\nservices following the training phase.\\nPrior research has shown the potential of LLMs in many NLP tasks, including specialized applications in domains such\\nas the medical and health sciences [12] and politics [13]. Moreover, after inventing the most sophisticated GPT model\\n[14], developing the state-of-the-art models (LLaMa and Bard [15]), and exploring their capabilities, such as Alpaca and\\nGPTHuggingface [16], it has become a crucial and impactful domain. As a result, a proper assessment of current LLM\\nresearch is becoming increasingly important, and prior research has shown the potential and superiority of LLMs in NLP\\ntasks. Nevertheless, few studies have thoroughly reviewed their work’s most recent LLM developments, possibilities,\\nand limitations. Despite the increasing number of studies on LLMs, there remains a scarcity of research focusing\\non their technical complexities, the LLMs taxonomy, architectures, API applications, domain-specific applications,\\neffective utilization, impact on society, and so on. Furthermore, the majority of the LLM review papers are not\\npeer-reviewed articles. So, the motivation of this paper is to explore the current review papers, identify their limitations,\\nand outline current state-of-the-art methodologies that have recently been created to address these challenges. However,\\nour main objective is to explore, learn, and assess LLMs across domains, evolutions, classifications, pre-trained models’\\narchitectures, resources, and real-time applications. Additionally, our comprehensive review discusses open issues and\\nchallenges associated with LLM, including safety, ethical, privacy, economic, and environmental considerations. In\\naddition, we present a set of guidelines to direct future research and development in the effective use of LLM. We hope\\nthat this study will contribute to a better understanding and use of LLMs. The list of contributions to this paper is as\\nfollows:\\n2'), Document(metadata={'source': './data/LLMS.pdf', 'page': 2}, page_content='A Review on LLM: Architectures, Applications, Open Issues and Challenges\\n• Providing an exhaustive overview of LLMs, including their evolutions, taxonomies, and transformer architec-\\ntures.\\n• Describing a comparative analysis of distinct pre-trained model architectures in LLMs along with their\\nindividual infrastructures.\\n• Explaining the impact of ML models in LLMs.\\n• Defining insight into the prospects of LLMs and their impact on society, as well as showcasing the applications\\nof LLMs in five practical domains, including bio-medical and healthcare, education, social media, business,\\nand agriculture.\\nThe remaining sections of the paper are organized as depicted in Figure 2.\\nFigure 2: Section organization of the review\\nIn Section 2, the literature review is discussed. Section 3 illustrates the history of LLMs; Section 4 explains the clear\\nconcept of the Transformer; Section 5 demonstrates the Methodology; Section demonstrates the pre-trained models’\\narchitectures, models comparison, and dataset; Section 7 describes the resources of LLMs; Section 8 demonstrates the\\ndomain-specific applications of LLMs; and Section 9 explains the societal impact of LLMs, Section 10 discuss the open\\nissues and challenges regarding this study and Section 11 finally concludes the paper.\\n2 Literature review\\nThe growing number of LLMs is an extraordinary development in AI. In recent years, the prevalence of these\\nmodels has skyrocketed, and numerous studies have been conducted to investigate and evaluate their expanding\\ncapabilities. Researchers from various fields have conducted exhaustive studies on the rise of LLMs, shedding light\\non their remarkable advancements, diverse applications, and potential to revolutionize tasks from text generation and\\ncomprehension to demonstrating reasoning skills. Collectively, these studies contribute to our comprehension of LLMs’\\nsignificant role in shaping the landscape of AI-driven language processing and problem-solving.\\nHuang et al. [17] presented a study on reasoning in large Language models that comprehensively summarizes the current\\nstate of LLM reasoning capabilities. It examines various aspects of reasoning in LLMs, such as techniques to enhance\\nand extract reasoning abilities, methodologies and criteria for assessing these abilities, insights from prior research,\\nand suggestions for future directions. The primary concern is the extent to which LLMs can demonstrate reasoning\\nskills. This paper aims to provide an in-depth and up-to-date examination of this topic, fostering fruitful discussions\\nand guiding future research in LLM-based reasoning. In another study, Zhao et al. [ 3] survey on LLM illustrates\\na comprehensive examination of the evolution and impact of LLMs in the field of artificial intelligence and natural\\nlanguage processing. It traces the historical journey from early language models to the recent emergence of pre-trained\\nlanguage models (PLMs) with billions of parameters. Notably, the paper discusses LLMs’ unique capabilities as they\\nscale in size, including in-context learning. The authors highlight the significant contributions of LLMs to the AI\\ncommunity and the launch of ChatGPT, a prominent AI chatbot powered by LLMs. The survey is structured around\\nfour key aspects of LLMs: pre-training, adaptation tuning, utilization, and capacity evaluation. Additionally, the paper\\nprovides insights into available resources for LLM development and identifies further research and development areas.\\nA recent study by Fan et al. [18] conducted a bibliometric review of LLM research from 2017 to 2023, encompassing\\nover 5,000 publications. The study aims to provide researchers, practitioners, and policymakers with an overview\\nof the evolving landscape of LLM research. It tracks research trends during the specified time period, including\\nadvancements in fundamental algorithms, prominent NLP tasks, and applications in disciplines such as medicine,\\n3'), Document(metadata={'source': './data/LLMS.pdf', 'page': 3}, page_content='A Review on LLM: Architectures, Applications, Open Issues and Challenges\\nengineering, the social sciences, and the humanities. In addition to highlighting the dynamic and swiftly changing nature\\nof LLM research, the study offers insights into their current status, impact, and potential in the context of scientific\\nand technological advancements. Another study by Chang et al., [ 19] focuses on the assessment of LMMs. Their\\nresearch examines the increasing prevalence of LLMs in academia and industry due to their exceptional performance\\nin various applications. It highlights the growing significance of evaluating LLMs at both the task and societal levels\\nin order to comprehend potential risks. The paper thoroughly analyzes LLM evaluation methods, focusing on three\\ncritical dimensions: what to evaluate, where to evaluate, and how to evaluate. It includes tasks such as natural language\\nprocessing, reasoning, medical applications, ethics, and education. The article examines evaluation methods and\\nbenchmarks for assessing LLM performance, emphasizing successful and unsuccessful cases. It underlines future\\nchallenges in LLM evaluation and emphasizes the significance of evaluating LLMs as a fundamental discipline to\\nsupport the development of more competent LLMs.\\nTable 1: Comparison between state-of-the-art research\\nPapers LLMLLMModelLLMAPI LLMDataset\\nDomainSpecificLLMTaxonomyLLMArchitectureLLMConfigurations\\nMLBasedDifferentiationScope Key Findings Methodologyand Approach\\nHuang et al. (2022)[17] ✓ X X X X X X X Reasoning inLLMs\\nAims to provide a criticalanalysis of LLM capabilities,methods for improving andevaluating reasoning,conclusions from earlierresearch, and future directions.\\nReview and analysisof reasoning abilitiesin LLMs\\nZhao et al. (2023)[3] ✓ X ✓ X ✓ X ✓ X Evolution andimpact of LLMs\\nExplore the historical journeyof LLMs, including pre-trainedlanguage models (PLMs) ,discussed about LLMs’ uniquecapabilities, insights into LLMdevelopment resources andhighlights significant contributionsof LLMs to AI and NLP researchareas.\\nSurvey and analysisof LLM evolutionand impact\\nFan et al. (2023)[18] ✓ X X X X X X X Bibliometricreview of LLMresearch\\nPresent a comprehensive overviewof LLM research from 2017 to 2023,tracking research trends, advancements,and provides insights into the dynamicnature of LLM research, and impact invarious domains.\\nBibliometric analysisof over 5,000 LLMpublications\\nChang et al. (2023)[19] ✓ X ✓ X ✓ X X X Assessmentof LLMs\\nInvestigate the methodologies employedin evaluating LLM programs, with aspecific focus on the aspects of what,where and how to conduct evaluationsand identified the potential risks andthe future challenge also.\\nSurvey and analysisof LLM evaluationapproaches\\nOURS ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Detailed reviewon LLMs\\nOur research investigated the history,resources, architectural configuration,domain-specific analysis, ml-baseddifferentiation, broad level of openissues, challenges, and future scope oflarge language models.\\nBroad review andanalysis of LLMsconsidering all thekey aspects\\nTable 1 illustrates the comparison between different review papers based on some critical factors such as LLM Model,\\nLLM API, LLM Dataset, Domain Specific LLM, Taxonomy, LLM Architecture, LLM Configurations, and ML Based\\nDifferentiation. Huang et al. [ 17] lack information on LLM API, LLM Dataset, Domain-Specific LLM, Taxonomy,\\nLLM Architecture, and LLM Configurations. In contrast, Zhao et al. [ 3] lack information on LLM API, Domain-\\nSpecific LLM, Taxonomy, LLM Architecture, and LLM Configurations. Moreover, Fan et al. [18] and Chang et al. [19]\\nlack information on LLM API, Domain-Specific LLM, Taxonomy, LLM Architecture, and LLM Configurations.\\nOur research shows more insights and discusses more features over the state-of-the-art studies mentioned above, given\\nthat it encompasses all the parameters in the table, thereby providing a holistic view of the state-of-the-art in LLM\\nresearch. While other studies concentrate on particular aspects of LLMs, such as their historical evolution, bibliometric\\ntrends, or evaluation methodologies, our research encompasses all of these aspects, providing a comprehensive\\nunderstanding of LLM capabilities. In addition, it focuses exclusively on the crucial aspect of reasoning abilities\\nin LLMs, making a substantial contribution to the field’s knowledge and making it an invaluable resource for LLM\\nresearchers and practitioners.\\n3 History of Large Language Models\\nLLMs refer to a category of AI models developed specifically to comprehend and produce human language [20]. LLMs\\nhave significantly transformed the field of AI and have been implemented in diverse areas, including education, commu-\\nnication, content generation, article composition, healthcare, research, entertainment, and information dissemination,\\namong others [20, 21]. This section provides a high-level overview of LLMs, including their development, training, and\\nfunctioning. Figure 3 depicts the history of language models.\\n4'), Document(metadata={'source': './data/LLMS.pdf', 'page': 4}, page_content='A Review on LLM: Architectures, Applications, Open Issues and Challenges\\nFigure 3: Brief history of language models.\\nIn the 1940s, Warren McCulloch and Walter Pitts introduced the world to the idea of artificial neural networks\\n(ANNs) [22]. Following this, the 1950s and 1960s saw the development of the first language models [ 23]. These\\nmodels included early neural networks as well as rule-based models. The processing of language was facilitated by\\ntheir utilization of precisely established linguistic rules and features [24].\\nStatistics-based models of language were created in the ’80s and ’90s. These models belong to a category of models\\nutilized in NLP and machine learning (ML) to capture and quantify the statistical patterns and correlations within\\nlanguage data [25]. The models employed probabilistic techniques to assess the probability of a sequence of words\\nor phrases inside a specific context. They were superior in terms of accuracy to early neural networks and rule-based\\nmodels, as they were able to process large amounts of data with ease [25].\\nDuring the mid-2000s, the field of NLP witnessed the introduction of word embeddings, recognized as a notable\\nbreakthrough, and subsequently acquired considerable attention [26]. This approach captures the semantic relationships\\namong words by representing them in a vector space [27]. Although not classified as LLMs, these embeddings have\\nsignificantly contributed to the progress of natural language comprehension and have set the path for developing more\\ncomplex models [26].\\nThe introduction of neural language models in the mid-2010s marked a significant advancement in large language\\nmodeling [28]. The initial neural language model to be introduced was the recurrent neural network language model\\n(RNNLM) in 2010 [29]. Its development aimed to capture the sequential dependencies present in textual data [ 30].\\nThe RNNLM demonstrated the capability to effectively capture the contextual information of words, resulting in the\\ngeneration of text that exhibits a higher degree of naturalness compared to earlier models [31].\\nIn the year 2015, Google unveiled the initial large neural language model that employed deep learning methodolo-\\ngies [32]. The technology was referred to as the Google Neural Machine Translation (GNMT) model [ 33]. The\\ndevelopment of this model signifies a notable progression in the field of machine translation [34]. The utilization of this\\nmodel resulted in enhanced translation accuracy and the generation of meaningful translations [33].\\nThe advancement of Language models persisted with the emergence of the Transformer model in the year 2017 [35].\\nThe transformer model has played a crucial role in the development of language models such as Bidirectional Encoder\\nRepresentations from Transformers (BERT) and Generative Pre-trained Transformers (GPT) [36]. The primary objective\\nbehind developing the Transformer model was to overcome the inherent constraints observed in earlier models such as\\nRNNs and LSTM networks [35].\\nThe introduction of transformer architecture-based BERT in 2018 by Google AI represents a noteworthy advancement in\\nthe domain of NLP [18]. Before the introduction of BERT, the preceding language model rooted in NLP had constraints\\n5'), Document(metadata={'source': './data/LLMS.pdf', 'page': 5}, page_content='A Review on LLM: Architectures, Applications, Open Issues and Challenges\\nin understanding contextual information due to its reliance on unidirectional language modeling. BERT was introduced\\nby Google as a solution to address this particular constraint [37]. The employed methodology involved the utilization of\\ndeep bidirectional representations, which were conditioned on both the left and right contexts across all layers [38].\\nIn 2018, GPT was developed by OpenAI, which was also a transformer-based architecture [ 39]. The introduction\\nof GPT-1 was a notable progression in the field of NLP. GPT-1 effectively produces contextually appropriate words,\\nshowcasing the transformative capabilities of transformers in significantly advancing NLP tasks. This proficiency is\\nattributed to its extensive training on many parameters, specifically 117 million [ 39]. The model underwent a two-step\\nprocedure consisting of unsupervised pre-training followed by supervised fine-tuning [21].\\nThe subsequent version of the GPT series, known as GPT-2, was designed to address the limitations observed in\\nGPT-1 [40]. Similar to GPT-1, GPT-2 was developed utilizing the Transformer architecture. In the year 2019, Alec\\nRadford introduced GPT-2, which was developed on a deep neural network consisting of 1.5 billion parameters [41].\\nThe GPT-2 model includes a transformer design, which incorporates self-attention processes to extract information from\\ndifferent positions within the input sequence [40]. The GPT-2 model has played a pivotal function in the advancement\\nof LLMs and the execution of NLP activities [42].\\nIn 2019, NVIDIA produced Megatron-LM, which is an LLM [43]. Similar to GPT, this model is built on the transformer\\narchitecture. The model possesses a total of 8.3 billion parameters, a notably bigger quantity compared to the parameter\\ncount of GPT-1 and GPT-2. The magnitude of this dimension facilitates the model’s capacity to acquire and produce\\nintricate linguistic structures [18].\\nIn the year 2020, OpenAI introduced GPT-3 as the successor to GPT-2 [ 40]. GPT-3 was trained on an extensive\\ncollection of textual data and demonstrated the ability to generate text that exhibited a high degree of coherence and\\nnaturalness. Similar to GPT-1 and GPT-2, this model also utilizes the Transformer architecture [21]. GPT-3 was trained\\non a deep neural network with an enormous 175 billion parameters, surpassing the size of any other LLMs available\\nat that particular time [18]. The ability to produce natural language text of superior quality with less fine-tuning is\\nfacilitated by sophisticated methodologies, including a more significant number of layers and a wider range of training\\ndata.\\nIn the year 2023, OpenAI introduced GPT-4, the subsequent version of their language model, following the achievements\\nof GPT-3 [21]. Similar to its predecessor, GPT-4 is a transformer-based model. The system has the capability to analyze\\nboth textual and visual data to produce textual outputs [ 18]. The system’s performance was assessed using a range\\nof standardized professional and academic examinations specifically intended for human test-takers [44]. GPT-4 has\\ngreater dimension and efficacy than its predecessor, as it can generate text that is even more comprehensive and exhibits\\na heightened level of naturalness [21].\\nThe development of large language models presents additional prospects for innovation, knowledge acquisition, and\\nexperimentation across diverse domains such as healthcare, education, research, etc. The utilization of AI and NLP in\\nthese models has significantly transformed how we engage with machine devices.\\n4 What is Transformer?\\nThe transformer architecture is considered the fundamental building block of LLMs. It is intended for neural networks\\nto handle sequential data effectively [10]. This architecture does not utilize recursion methods. Instead, it employs an\\nattention method to determine global input-output dependencies. It has resulted in novel model sizes and performance\\nlevels, allowing for substantially increased parallelization and reduced training times in NLP. Furthermore, it can take\\ninput of varying lengths and change its attention depending on the length of the sequence. As a result, it became the\\ngo-to architecture in many fields, frequently replacing sophisticated recurrent or convolutional neural networks with a\\nfar more efficient structure [35]. In this regard, it is especially important for LLM applications. Figure 4 depicts the\\ntransformer model’s architecture. Transformer architecture consists of seven key components. A demonstration of each\\nof the components is shown below [10].\\n4.1 Inputs and Input Embeddings\\nThe ML models use user-entered tokens as training data, while it can only process numeric information. Thus, it\\nis necessary to transform these textual inputs into a numerical format known as \"input embeddings.\" These input\\nembeddings are numerical representations of words, which ML models may subsequently process. These embeddings\\nfunction similarly to a dictionary, assisting the model in understanding the meaning of words by arranging them in\\na mathematical space where comparable phrases are situated close together. The model is trained to generate these\\n6'), Document(metadata={'source': './data/LLMS.pdf', 'page': 6}, page_content='A Review on LLM: Architectures, Applications, Open Issues and Challenges\\nFigure 4: Architecture of a Transformer model\\nembeddings so that vectors of the same size represent words with similar meanings. Figure 4A illustrates the input and\\ninput embeddings.\\n4.2 Positional Encoding\\nThe order of words in a sentence is essential in the NLP field for identifying the statement’s meaning. In general, in terms\\nof neural networks, they do not fundamentally grasp the sequence of inputs. To remedy the issue, positional encoding\\nmay be utilized to encode each word’s location in the input sequence as a collection of integers. The Transformer model\\naccepts these integers and input embeddings while adding positional encoding to the transformer architecture allows\\nGPT to grasp sentence word order better and provide grammatically accurate and semantically relevant output. The\\npositional encoding part is displayed in Figure 4B.\\n4.3 Encoder\\nThe encoder is part of the neural network that processes the input text and generates a series of hidden data. Then, it\\nuses a series of self-attention layers, which we can think of as voodoo magic, to make several hidden states that describe\\nthe input text at different levels of abstraction. In the transformer, the encoder is used in more than one layer. This\\nsection is depicted in Figure 4C comprehensively.\\n4.4 Outputs (shifted right)\\nDuring the training process, the decoder acquires the ability to predict the next word by analyzing the previous words.\\nIn this case, the output sequence is shifted by one position to the right. Consequently, the decoder is able to use the\\nwords that came before it. Additionally, the GPT (GPT-3) is also trained on a massive amount of text data, that helps\\nit generate sense while writing something. Besides, several corpus including the Common Crawl web corpus, the\\nBooksCorpus dataset, and the English Wikipedia are also used during the common issue. Figure 4D highlights the\\ntransformer’s outputs (shifted right) module.\\n4.5 Output Embeddings\\nInput embeddings, which contain text and are not recognized by the model. Therefore, the output must be converted\\nto a format known as \"output embedding.\" Like input embeddings, output embeddings undergo positional encoding,\\n7'), Document(metadata={'source': './data/LLMS.pdf', 'page': 7}, page_content='A Review on LLM: Architectures, Applications, Open Issues and Challenges\\nenabling the model to understand the order of words in a sentence. In machine learning, the loss function evaluates\\nthe difference between a model’s prediction and the objective value. Loss functions are essential for complex GPT\\nlanguage models. The loss function modifies a portion of the model to increase accuracy by reducing the discrepancy\\nbetween predictions and targets. The change improves the overall performance of the model. The loss function is\\ncalculated during training, and the model parameters are modified. In the inference process, the output text is created by\\nmapping the predicted probability of each token in the model to the corresponding token in the vocabulary. The output\\nembedding part is illustrated in Figure 4E.\\n4.6 Decoder\\nThe decoder processes positionally encoded input and output embedding. Model decoders create output sequences from\\nencoded input sequences while the decoder learns to predict the next word from previous words during the training\\nperiod. In addition, the GPT’s decoder generates natural language text by utilizing encoder context and input sequence.\\nHowever, the transformers employ many decoder layers like encoders. Figure 4F demonstrates the decoder component\\nof a transformer.\\n4.7 Linear Layer and Softmax\\nThe linear layer maps to the higher-dimensional space once the decoder has generated the output embedding. This step\\nis required to convert the output embedding into the original input space. The softmax function generates a probability\\ndistribution for each output token in the developed vocabulary, allowing us to generate probabilistic output tokens.\\nFigure 4G shows the process by which the features are propagated through a linear layer, followed by the activation of\\nthe accurate output probability using the softmax activation function.\\n5 Methodology\\nThe research materials utilized in this study have been obtained from reputable scholarly journals and conferences,\\nspanning the time frame between January 2020 and August 2023. The search and selection process was carried out\\nusing the Google Scholar platform. Our primary objective is to acquire relevant articles written in the English language.\\nA compilation of scholarly research publications has been selected, including a wide array of esteemed academic\\nsources such as IEEE Xplore, ScienceDirect, ACM Digital Library, Wiley Online Library, Springer Link, MDPI, and\\npatents. Table 2 depicts the electronic database that was utilized to conduct a comprehensive search for papers relevant\\nto this research.\\nTable 2: Electronic database search\\nElectronic\\nDatabase\\nType URL\\nIEEE\\nXplore\\nDigital Li-\\nbrary\\nhttps://ieeexplore.ieee.org/Xplore/home.jsp\\n(accessed on 18 September, 2023)\\nSpringer Digital Li-\\nbrary\\nhttps://www.springer.com/gp (accessed on 18\\nSeptember, 2023)\\nGoogle\\nScholar\\nSearch En-\\ngine\\nhttps://scholar.google.com.au (accessed on 18\\nSeptember, 2023)\\nScience\\nDirect—\\nElsevier\\nDigital Li-\\nbrary\\nhttps://www.sciencedirect.com (accessed on 18\\nSeptember, 2023)\\nMDPI Digital Li-\\nbrary\\nhttps://www.mdpi.com (accessed on 18 September,\\n2023)\\nACM Digital Li-\\nbrary\\nhttps://www.researchgate.net (accessed on 18\\nSeptember, 2023)\\nAdditionally, the electronic databases are accompanied by their respective URLs. To perform an extensive search of the\\narticles, a diverse set of Search Queries were utilized, incorporating terms such as \"LLM AND machine learning OR\\ndeep learning OR models\", \"LLM AND machine learning OR deep learning OR API\", \"LLM AND machine learning\\nOR deep learning OR Dataset\", and \"LLM” AND machine learning OR deep learning OR tools\". Table 3 presents the\\nSearch Queries (SQ) employed in this study. Figure 5 illustrates the comprehensive search running on Google Scholar,\\nemploying the given search queries (SQs) to identify relevant scholarly articles for the study. In the beginning, a total of\\n8'), Document(metadata={'source': './data/LLMS.pdf', 'page': 8}, page_content='A Review on LLM: Architectures, Applications, Open Issues and Challenges\\nTable 3: Search queries used for the review paper.\\nSearch Queries (SQ)\\nSQ1 “LLM” AND machine learning OR deep learn-\\ning OR models\\nSQ2 “LLM” AND machine learning OR deep learn-\\ning OR API\\nSQ3 “LLM” AND machine learning OR deep learn-\\ning OR Dataset\\nSQ4 “LLM” AND machine learning OR deep learn-\\ning OR tools\\nFigure 5: Flow diagram of systematic review\\n212 scholarly publications were collected from various academic repositories. The inclusion and exclusion criteria for\\nthe articles included in this study are presented in Table 4.\\nThe table outlines the addition of five inclusion criteria (IC) and four exclusion criteria (EC) for selecting the relevant\\narticles of the study. In addition, we have eliminated the articles that are not relevant to our research. Following the\\nimplementation of the filtering procedure, the total count of papers was diminished to 101. Next, we proceeded to\\nexclude journal and conference papers that were scored poorly in terms of quality. In the end, we achieved a further\\nreduction in our compilation by deleting the redundant articles. For their precise alignment with the primary objective\\nof our study, a total of 55 papers were selected for inclusion in this article.\\n6 Large Language Models\\nLarge language models (LLMs) refer to a specific type of AI algorithm that holds the capability to execute a diverse\\nrange of NLP operations. The most common tasks entail text generation, text analysis, translation, sentiment analysis,\\nquestion answering, and other related functions. GPT-3, GPT-4, PaLM, and LaMDA are extensively used transformer-\\nbased LLM models trained on a large amount of textual data. In terms of architectural properties, these models show\\nvariations in size and depth. For example, GPT-3 generates parameters of 175 billion, distributed across 96 levels,\\nwhile PaLM has an even larger parameter number of 540 billion, organized across 106 layers. All of these models\\n9'), Document(metadata={'source': './data/LLMS.pdf', 'page': 9}, page_content='A Review on LLM: Architectures, Applications, Open Issues and Challenges\\nTable 4: Inclusion and exclusion criteria.\\nList of Inclusion and Exclusion Criteria\\nInclusion Criteria (IC)\\nIC1 Should contain at least one of the keywords\\nIC2 Must be included in one of the selected databases\\nIC3 Published within the last ten years (2014–2023)\\nIC4 Publication in a journal, conference is required\\nIC5 The research being examined should have a match-\\ning title, abstract, and full text\\nExclusion Criteria (EC)\\nEC1 Redundant items\\nEC2 Whole text of paper cannot be taken\\nEC3 Purpose of the paper is not related to LLM\\nEC4 Non-english documents\\nhave distinct configurations. The configurations of GPT-3 and PaLM differ in terms of their techniques for generating\\noutput. LLMs have evaluated several datasets within Wikipedia, code repositories, books, question sets, and social\\nmedia data. They have demonstrated their ability to execute diverse activities successfully. Consequently, LLMs have\\ndrawn significant attention for their effective contribution in different domains, including education, healthcare, media\\nmarketing, and other customer services. A particular LLM program has superior performance in a specific domain\\ncompared to others, such as GPT-3, which has gained recognition for its proficiency in generating text styles, whereas\\nLaMDA demonstrates superior performance in providing accurate responses to factual inquiries. LLMs are an emerging\\ntechnological innovation that holds the potential to bring about transformative changes across various sectors.\\nIn this section, the architectural overview of LLMs is discussed initially in the subsection 6.1. Then, we presented the\\ncomparison between configurations of LLMs in the next subsection 6.2, and finally, in subsection 6.3, the datasets\\nutilized for training the LLMs are discussed.\\n6.1 Architectural Overview of Large Language Models\\nIn Table 5, a description and architecture of LLMs such as GPT-1, BERT, RoBERta, and T5 are presented.\\nThis table will assist researchers in selecting the optimal model for a natural language processing task. GPT-1, BERT\\nbase, and BERT large contain 12, 12, and 24 layers, correspondingly, in the larger language model. RoBERta is an\\nenhanced variant of BERT, while T5 is a decoder and encoder transformer. Diagram illustrating BERT’s input token\\nprocessing, context-aware embedding, and masked language modeling tasks, where the masked words are intended to\\npredict the model. T5 demonstrates the sequential layers of the transformer model, including the feedforward neural\\nnetwork, and self-attention. It explains how information flows and structures text. GPT-1 passes data input embedding\\nand positional encoding through multiple transformer layers.\\n6.2 Comparison Between Configurations of LLMs\\nTable 6 provides an extensive overview of various Large Language Models (LLMs), highlighting their configuration\\ndetails and optimization settings.\\nThese LLMs have played a crucial role in advancing natural language comprehension and generation tasks, making\\nthem a focal point in artificial intelligence and natural language processing. This analysis compares and contrasts these\\nLLMs based on critical parameters, including model size, learning rate, category, activation function, batch size, bias,\\nnumber of layers, optimizer, number of attention heads, hidden state size, dropout rate, and maximum training context\\nlength. GPT-4 stands out as the most prominent model on display, with a staggering 1.8 trillion parameters. GPT-1,\\ndespite being lesser with 125 million parameters, demonstrates the significant development of LLM over the years. An\\nincreased number of parameters in LLM enhances the model’s ability to comprehend intricate patterns and produce\\ntext that is more contextually appropriate and reminiscent of human language. GPT3’s selection of a modest learning\\nrate of 6 is notable, which highlights the significance of cautious hyperparameter selection. Models are categorized as\\nCausal decoder (CD), Autoregressive (AR), Encoder-decoder (ED), and Prefix decoder (PD) to illustrate architectural\\ndiversity. Activation functions vary, influencing the models’ expressive strength from GeLU in GPT-3 to SwiGLU in\\nLLaMA and LLaMA-2. All versions of GPT employ the GeLU as its activation function as it mitigates the vanishing\\ngradient problem and facilitates the generation of smoother gradients throughout the training process. The utilization of\\nSwiGLU as the activation function is observed in models such as PaLM and LLaMA versions 1 and 2, as it has gating\\n10'), Document(metadata={'source': './data/LLMS.pdf', 'page': 10}, page_content='A Review on LLM: Architectures, Applications, Open Issues and Challenges\\nTable 5: Architectural overview of different LLMs\\nModel Description Architecture\\nGPT-1 [45]\\nTwelve-level decoder\\ntransformer that uses\\ntwelve masked\\nself-focusing heads.\\nBERT [11]\\nBERT is a transformer\\narchitecture. It has two model\\nsizes. BERT base has 12 layers\\nin encoder stack and BERT\\nLarge has 24 layers in encoder\\nstack.\\nRoBERTa [46] Optimized version of\\nBERT model.\\nT5 [47]\\nThe model consists of an encoder\\nand a decoder transformer, which\\nhas many layers.\\nmechanisms that enhance its ability to capture intricate correlations within the data. Models like BERT, OPT, and T5\\nuse ReLU as the activation function. The Formula of these activation functions are given below [67, 68]:\\nReLU(x) = max(0, x) =f(x) =\\n\\x1ax, if x ≥ 0\\n0, if x <0 (1)\\nGeLU(x) = 0.5x(tanh[\\np\\n2/π(x + 0.44715x3)]) (2)\\nSwiGLU(x) =x.Sigmoid(βx).xV (3)\\n11'), Document(metadata={'source': './data/LLMS.pdf', 'page': 11}, page_content='A Review on LLM: Architectures, Applications, Open Issues and Challenges\\nTable 6: Various LLMs with configuration details and optimization settings (Here, LR = learning rate, CG = Category,\\nAF = the activation function, bs = batch size, NL = the number of layers, NAH = the number of attention heads, SHS\\n= the size of the hidden states, MCLDT = the maximum context length during training, CD = causal decoder, ED =\\nencoder-decoder, PD = prefix decoder, and AR = autoregressive)\\nModel Size LR CG AF BS Bias NL Optimizer NAH SHS Dropout MCLDT\\nGPT-4 [48] 1.8 T − CD GeLU − Yes 120 Adam 120-150 20000 − 32768\\nGPT-3 [49] 175B 6×10−5 CD GeLU 32K-3200K Yes 96 Adam 96 12288 − 2048\\nGPT-2 [50] 1.5B 1×10−4 AR GeLU 16K-64K Yes 48 Adam 24 1280 0.1 1024\\nGPT-1 [45] 125M 1×10−4 AR GeLU 16K-64K Yes 12 Adam 12 768 0.1 512\\nBARD [51] 340M − − ReLU 64K Yes 24 − 24 768 − 512\\nBERT [45] 340M 1×10−5 − ReLU 16K-64K Yes 24 Adam 16 1024 0.1 512\\nPanGU-α[52] 207B 2×10−5 CD GeLU − Yes 64 Adam 128 16384 − 1024\\nBLOOM [53] 176B 6×10−5 CD GeLU 4000K Yes 70 Adam 112 14336 0 2048\\nGalactica [54] 120B 7×10−6 CD GeLU 2000K No 96 AdamW 80 10240 0.1 2048\\nOPT [55] 175B 1.2×10−4 CD ReLU 2000K Yes 96 AdamW 96 12288 0.1 2048\\nChinchilla [56] 70B 1×10−4 CD − 1500K-3000K − 80 AdamW 64 8192 − −\\nFalcon [57] 40B 1.85×10−4 CD GeLU 2000K No 60 AdamW 64 8192 − 2048\\nT5 [58] 11B 1×10−2 ED ReLU 64K No 24 AdaFactor 128 1024 0.1 512\\nLLaMA [59] 65B 1.5×10−4 CD SwiGLU 4000K No 80 AdamW 64 8192 − 2048\\nLLaMA-2 [60] 70B 1.5×10−4 CD SwiGLU 4000K No 80 AdamW 64 8192 − 4096\\nMT-NLG [61] 530B 5×10−5 CD − 64K-3750K − 105 Adam 128 20480 − 2048\\nJurassic-1 [62] 178B 6×10−5 CD GeLU 32K-3200K Yes 76 − 96 13824 − 2048\\nGopher [63] 280B 4×10−5 CD − 3000K-6000K − 80 Adam 128 16384 − 2048\\nGLM-130B [64]130B 8×10−5 PD GeGLU 400k-8250K Yes 70 AdamW 96 12288 0.1 2048\\nLaMDA [65] 137B − CD GeGLU 256K − 64 − 128 8192 − −\\nPaLM [66] 540B 1×10−2 CD SwiGLU 1000K-4000K No 118 Adafactor 48 18432 0.1 2048\\nDifferent models have different batch sizes, with GLM-130B’s larger batch size of 400k-8250K indicating enhanced\\ntraining efficacy. In addition, the presence or absence of biased terms in models, such as Falcon, T5, LLaMA 1,2, and\\nGalactica’s \"No,\" highlights the complexity of the choices made. From 12 for GPT-1 to 118 for PaLM, the number\\nof layers affects a model’s ability to capture intricate patterns. Optimizers are also diverse, with Adam, AdamW, and\\nAdaFactor playing crucial roles. All GPT variants employ Adam as the optimizer, although models such as Galactica,\\nOPT, and Falcon utilize AdamW as their optimizer. Both T5 and PaLM models utilize the Adafactor optimizer in their\\nrespective architectures. With 530 billion parameters, models like MT-NLG test scalability limits, while others like\\nChinchilla remain relatively compact. These variations highlight the significance of selecting models and configurations\\nthat are tailored to particular tasks, with performance, computational resources, and task requirements playing a central\\nrole.\\nThe number of attention heads also exhibits variation across different models. GPT-1 is equipped with a total of 12\\nattention heads, whilst GPT-4 boasts a much larger number of attention heads, ranging from 120 to 150 within its model.\\nThe additional number of attention heads in the LLM enables the model to concurrently attend to several segments\\nof the input sequence, hence expediting the model’s training process. In order to enhance the efficacy of the LLMs,\\nresearchers employ diverse dimensions for the hidden states within their model. The larger dimensions of the hidden\\nstate enable the capturing of complex patterns within the text. Both GPT 4 and MT-NLG employ hidden state sizes of\\napproximately 20,000, which is significantly greater in comparison to the hidden state sizes of other LLMs included in\\nthe table. Certain LLM models incorporate a dropout value of 0.1 to prevent overfitting issues, whereas others do not\\nemploy any dropout value. With 530 billion parameters, models like MT-NLG test scalability limits, while others like\\nChinchilla remain relatively compact. These variations highlight the significance of selecting models and configurations\\nthat are tailored to particular tasks, with performance, computational resources, and task requirements playing a central\\nrole.\\n6.3 Comparison Between Datasets of LLMs\\nDifferent LLM utilized different datasets for the training phase, distinguishing the models from one another. A concise\\noverview of the datasets is provided in this section. The datasets used to train various large language models (LLMs)\\nand their compatibility with each model are detailed in Table 7.\\n12'), Document(metadata={'source': './data/LLMS.pdf', 'page': 12}, page_content='A Review on LLM: Architectures, Applications, Open Issues and Challenges\\nTable 7: Dataset for large language models\\nDataset→ Webpages ConversationData Books and News Scientific Data Code\\nLLM↓ C4 OpenWebTextWikipediathe Pile -StackExchangeBookCorpusGutenbergCC-Stories-RCC-NEWESREALNEWsthe Pile -ArXiv\\nthe Pile -PubMedAbstractsBigQuerythe Pile -GitHub\\nT5 [58] ✓ ✓ ✓ X X X X X X X X X XFalcon [57]✓ ✓ ✓ X X X X X X X X X XLLaMA [59]✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓GPT-3 [49]✓ ✓ ✓ X ✓ ✓ ✓ ✓ ✓ X X X XGPT-4 [48]✓ ✓ ✓ X ✓ ✓ ✓ ✓ ✓ X X X XMT-NLG [61]✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓Gopher [63]✓ ✓ ✓ X ✓ ✓ ✓ ✓ ✓ X X ✓ ✓Chinchilla [56]✓ ✓ ✓ X ✓ ✓ ✓ ✓ ✓ X X ✓ ✓GLaM [69] ✓ ✓ ✓ X ✓ ✓ ✓ ✓ ✓ X X X XPaLM [66] ✓ ✓ ✓ X ✓ ✓ ✓ ✓ ✓ X X ✓ ✓LaMDA [65]✓ ✓ ✓ X X X X X X ✓ ✓ ✓ ✓Galactica [54]✓ ✓ ✓ X X X X X X ✓ ✓ ✓ ✓GPT-NeoX [70]✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓CodeGen [71]✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓AlphaCode [72]X X X X X X X X X X X ✓ ✓Size 800GB 38GB 21GB 800GB 5GB - 31GB 78GB 120GB 800GB800GB - 800GB\\nSource CommonCrawl(April 2019)RedditLinks(March 2023)Wikipedia(March 2023)Other(Dec 2020)Books(Dec 2015)Books(Dec 2021)CommonCrawl(Sep 2019)CommonCrawl(Feb 2019)CommonCrawl(April 2019)Other(Dec 2020)Other(Dec 2020)Codes(March 2023)Other(Dec 2020)\\nTable 7 demonstrates that datasets have been divided into multiple categories: webpages, conversation data, literature\\nand news, scientific data, and code. This classification enables us to comprehend the variety of data sources that\\ncontribute to LLM training. C4, OpenWebText, and Wikipedia are examples of datasets that belong to the \"Webpages\"\\ncategory. At the same time, BookCorpus, Gutenberg, CC-Stories-R, CC-NEWES, and REALNEWS are examples of\\ndatasets that belong to the \"Books and News\" category. These categories reflect the richness and diversity of text data\\nused to train LLMs, including web content, novels, news articles, scientific literature, and code.\\nFrom the ✓, it can be seen that LLaMA has been trained on a wide range of data sources, with significant exposure\\nto webpages (87%), conversation data (5%), books and news (2%), scientific data (3%), and code (5%). This makes\\nLLaMA a versatile model suitable for a wide array of natural language processing tasks that involve these data types. In\\ncontrast, platforms such as GPT-3 and AlphaCode have restricted data exposure. GPT-3 is proficient with web pages\\n(84%), literature, and news (16%) but requires additional instruction with conversation data, scientific data, and code.\\nAlphaCode, as its name suggests, is solely focused on code (100%) and does not utilize any other data sources. These\\nfindings highlight the significance of selecting the appropriate LLM based on the task’s requirements. AlphaCode is the\\nmodel of choice for code-related tasks, whereas LLaMA excels in diverse text data applications. In addition, the \"Size\"\\nand \"Source\" columns of Table 7 offer additional context. The size of datasets ranges from 5GB (BookCorpus) to a\\nmassive 800GB (several datasets), indicating the sheer magnitude of data required to train these LLMs. The source\\ninformation reveals when and where the data were collected, which is essential for comprehending the training data’s\\ntemporal relevance and potential biases. Table 7 provides a multitude of information regarding the datasets used to train\\nLLMs and how each model leverages these datasets. This information is invaluable for natural language processing\\nresearchers, developers, and practitioners, as it enables them to make informed decisions about which LLM to use for\\nspecific tasks and casts light on the breadth and depth of data that powers these cutting-edge language models.\\n7 Resources of Large Language Models\\nLarge Language Models (LLMs) have a wide range of potential applications and resources available for their develop-\\nment, deployment, and utilization. In Figure 6, we present an LLM taxonomy that categorizes Large Language Models\\ninto two main branches: those based on pre-trained models and those based on APIs.\\n13'), Document(metadata={'source': './data/LLMS.pdf', 'page': 13}, page_content='A Review on LLM: Architectures, Applications, Open Issues and Challenges\\nFigure 6: Taxonomy of LLM\\nThis taxonomy allows for a comprehensive exploration of these two distinct aspects of Large Language Models. Here\\nare some key resources presented which are associated with LLMs:\\n7.1 Pretrained Models\\nPretrained language models play a pivotal role in natural language processing due to their ability to encapsulate broad\\nlanguage understanding and generation skills gleaned from diverse text sources. They offer a substantial advantage by\\nminimizing the computational resources and data required for fine-tuning specific tasks. There are some of the most\\ncommon pre-trained LLM models, which have been depicted in Table 8.\\n7.1.1 Generative Pretrained Transformer (GPT)\\nGenerative Pre-trained Transformer [49] is an influential breakthrough in artificial intelligence, particularly in natural\\nlanguage processing (NLP). Developed by OpenAI, GPT leverages the Transformer architecture and extensive pre-\\ntraining on vast internet text data to achieve a deep understanding of human language. This generative model excels\\nat tasks like text generation, translation, question answering, and more, making it a versatile tool across various NLP\\ndomains. GPT’s capacity to capture intricate language patterns and context, coupled with its iterative improvements, has\\nprofoundly impacted academia and industry, revolutionizing the landscape of language understanding and generation.\\n7.1.2 BERT\\nBERT [11], short for \"Bidirectional Encoder Representations from Transformers,\" is a language model with a distinctive\\napproach. Unlike previous models, BERT is designed to pre-train deep bidirectional representations from unlabeled text\\nby considering both left and right context in all layers. This pre-trained BERT model can be fine-tuned with minimal\\nadjustments to create cutting-edge models for various tasks like question answering and language inference, eliminating\\nthe need for extensive task-specific modifications. BERT is both conceptually straightforward and remarkably effective,\\nachieving state-of-the-art results on eleven different natural language processing tasks. Notable accomplishments\\ninclude raising the GLUE score to 80.5% (an impressive 7.7% absolute improvement), boosting MultiNLI accuracy to\\n86.7% (a 4.6% absolute improvement), and significantly improving SQuAD v1.1 question answering Test F1 to 93.2 (a\\n1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (a remarkable 5.1 point absolute improvement).\\nIn our analysis, we have exclusively considered versions of BERT (Bidirectional Encoder Representations from\\nTransformers) that are inherently Large Language Models (LLMs). Specifically, we focused on variants of BERT\\nthat are pre-trained on extensive text corpora and possess the characteristics of LLMs, enabling them to understand\\nand generate natural language comprehensively. This deliberate choice ensures that the models we have included in\\n14'), Document(metadata={'source': './data/LLMS.pdf', 'page': 14}, page_content='A Review on LLM: Architectures, Applications, Open Issues and Challenges\\nTable 8: Description of Language Models\\nModel Name Description Key Features Training\\nData\\nFine-Tuning\\nData\\nFine-Tuning\\nTasks\\nApplications\\nGPT (Genera-\\ntive Pretrained\\nTransformer)\\n[49]\\nTransformative\\nLLM by OpenAI\\nfor versatile NLP\\ntasks.\\nExtensive pre-\\ntraining, deep\\nlanguage un-\\nderstanding,\\niterative improve-\\nments, impact on\\nacademia/industry\\nInternet text\\ndata\\nCustom\\ndatasets\\nText gen-\\neration,\\ntranslation,\\nQA, and more\\nChatbots,\\ncontent gen-\\neration, NLP\\ndomains\\nBERT (Bidi-\\nrectional\\nEncoder Rep-\\nresentations\\nfrom Trans-\\nformers) [11]\\nGoogle AI’s NLP\\nmodel excelling\\nwith bidirectional\\ncontext learning.\\nDeep bidi-\\nrectional rep-\\nresentations,\\nconceptually\\nstraightfor-\\nward, minimal\\ntask-specific\\nadjustments\\nBookCorpus,\\nWikipedia\\nTask-specific\\ndatasets\\nVarious NLP\\ntasks\\nQuestion\\nanswering,\\nlanguage\\ninference\\nRoBERTa\\n[46]\\nBERT-based\\nmodel with\\nrefined hyperpa-\\nrameters.\\nSignificance of\\ndesign decisions,\\npublicly available,\\ntop-tier NLP\\nresults\\nBookCorpus,\\nWikipedia\\nTask-specific\\ndatasets\\nVarious NLP\\ntasks\\nBenchmark\\nimprove-\\nments, re-\\nsearch\\nXLNet [73] Combines au-\\ntoregressive\\npretraining with\\nbidirectional\\ncontext learning.\\nBidirectional con-\\ntext learning, ver-\\nsatile approach\\nInternet text\\ndata\\nTask-specific\\ndatasets\\nDiverse NLP\\ntasks\\nResearch, ap-\\nplications\\nSpeech-\\nXLNet [74]\\nUnsupervised\\nacoustic model\\nwith robust regu-\\nlarization.\\nRobust regular-\\nizer, improved\\nrecognition accu-\\nracy\\nSpeech\\ndatasets\\nTIMIT, WSJ\\ndatasets\\nSpeech recog-\\nnition\\nSpeech recog-\\nnition systems\\nDialogXL\\n[75]\\nImproved dia-\\nlogue handling\\nwith dialog-aware\\nself-attention.\\nEnhanced conver-\\nsation modeling,\\noutperforms base-\\nlines\\nInternet text\\ndata\\nDialogue\\ndatasets\\nDialogue un-\\nderstanding\\nChatbots, cus-\\ntomer support\\nT5 (Text-to-\\nText Transfer\\nTransformer)\\n[47]\\nGoogle’s unified\\ntext-to-text NLP\\nmodel.\\nUnified frame-\\nwork, extensive\\npre-training,\\nversatile tool\\nInternet text\\ndata\\nTask-specific\\ndatasets\\nText classifica-\\ntion, transla-\\ntion, and more\\nLanguage\\ntranslation,\\nsummariza-\\ntion\\nBioGPT [76] Specialized\\nbiomedical LLM\\nwith state-of-the-\\nart results.\\nBiomedical liter-\\nature pretraining,\\nexcels in biomedi-\\ncal tasks\\nBiomedical lit-\\nerature\\nBiomedical\\ndatasets\\nBiomedical\\ntext analysis\\nBiomedical\\ntext analysis,\\nresearch\\n15'), Document(metadata={'source': './data/LLMS.pdf', 'page': 15}, page_content='A Review on LLM: Architectures, Applications, Open Issues and Challenges\\nour study harness the full spectrum of language understanding and generation capabilities, thereby aligning with the\\ncore objective of our research in exploring the impact and advancements of LLMs in the field of natural language\\nprocessing. Non-LLM versions of BERT or those with significantly reduced model sizes were excluded from our\\nanalysis to maintain consistency and relevance in our investigation of the transformative potential of Large Language\\nModels.\\n7.1.3 RoBERTa\\nRoBERTA [46] is a study that replicates the BERT pretraining approach outlined by Devlin et al. in 2019. In this\\nstudy, we meticulously assess the influence of various critical hyperparameters and training data sizes. It’s worth noting\\nthat BERT was initially trained with room for improvement, yet it can now perform on par with or even surpass the\\nperformance of subsequent models that have been published. As a result, RoBERTa achieves top-tier results in GLUE,\\nRACE, and SQuAD evaluations. These outcomes underscore the significance of design decisions that were previously\\noverlooked and prompt inquiries into the origins of recently reported advancements. We have made our models and\\ncode available for public use.\\n7.1.4 XLNet\\nXLNet [73] represents a versatile autoregressive pretraining approach that achieves bidirectional context learning by\\noptimizing expected likelihood across all possible permutations of factorization orders. It addresses the constraints\\nof BERT through its autoregressive design and incorporates insights from Transformer-XL, a leading autoregressive\\nmodel. In practical experiments with consistent conditions, XLNet consistently surpasses BERT on 20 diverse tasks,\\nfrequently by a substantial margin. These tasks encompass question answering, natural language inference, sentiment\\nanalysis, and document ranking, among others.\\n7.1.5 Speech-XLNet\\nSpeech-XLNet [74] is a method for training unsupervised acoustic models to learn speech representations using a Self-\\nAttention Network (SAN) and subsequently fine-tuning it within the hybrid SAN/HMM framework. Our hypothesis is\\nthat by rearranging the order of speech frames, the permutation technique in Speech-XLNet acts as a robust regularizer,\\nencouraging the SAN to make inferences by prioritizing global structures through its attention mechanisms. Moreover,\\nSpeech-XLNet enables the model to explore bidirectional contexts, enhancing the effectiveness of speech representation\\nlearning. Experimental results on TIMIT and WSJ datasets demonstrate that Speech-XLNet significantly enhances\\nthe performance of the SAN/HMM system in terms of both convergence speed and recognition accuracy compared to\\nsystems trained from randomly initialized weights. Our best models achieve an impressive relative improvement of\\n11.9% and 8.3% on the TIMIT and WSJ tasks, respectively. Notably, the top-performing system achieves a phone error\\nrate (PER) of 13.3% on the TIMIT test set, which, to the best of our knowledge, is the lowest PER achieved by a single\\nsystem.\\n7.1.6 DialogXL\\nDialogXL [75] introduces improvements to handle longer historical context and multi-party structures in dialogues.\\nFirstly, it modifies the way XLNet handles recurrence, moving from segment-level to utterance-level, which enhances\\nits ability to model conversational data effectively. Secondly, it incorporates dialog-aware self-attention instead of\\nthe standard self-attention in XLNet, allowing it to capture important dependencies within and between speakers. A\\ncomprehensive set of experiments is conducted on four ERC benchmarks, comparing DialogXL with mainstream\\nmodels. The experimental findings consistently demonstrate that our model surpasses the baseline models across all\\ndatasets. Additionally, we perform other experiments, including ablation studies and error analyses, which confirm the\\nsignificance of DialogXL’s critical components.\\n7.1.7 T5\\nT5 [47], or \"Text-to-Text Transfer Transformer,\" is a groundbreaking large language model developed by Google\\nResearch, revolutionizing natural language processing (NLP). T5’s innovation lies in framing all NLP tasks as text-\\nto-text tasks, simplifying the NLP pipeline and unifying various tasks under a single framework. Built upon the\\nTransformer architecture, T5 utilizes multi-head self-attention to capture intricate language relationships. Its extensive\\npre-training on vast text data, followed by fine-tuning on specific tasks, empowers T5 to excel in text classification,\\ntranslation, summarization, question answering, and more. With consistently state-of-the-art results across NLP\\nbenchmarks, T5 has reshaped the field, offering researchers and developers a versatile tool for comprehensive language\\nunderstanding and generation tasks.\\n16'), Document(metadata={'source': './data/LLMS.pdf', 'page': 16}, page_content='A Review on LLM: Architectures, Applications, Open Issues and Challenges\\n7.1.8 BioGPT\\nBioGPT [76] is a specialized Transformer-based language model, pre-trained using extensive biomedical literature. We\\nconducted evaluations across six biomedical natural language processing tasks and found that our model consistently\\noutperforms previous models in most cases. Notably, we achieved F1 scores of 44.98%, 38.42%, and 40.76% on\\nBC5CDR, KD-DTI, and DDI end-to-end relation extraction tasks, respectively, and set a new accuracy record of 78.2%\\non PubMedQA. Furthermore, our investigation into text generation highlights BioGPT’s proficiency in generating\\ncoherent descriptions for biomedical terms within the literature.\\nIn summary, pre-trained LLMs are foundational in NLP, providing a starting point for various applications without the\\nneed for extensive training from scratch. They are widely used and have democratized access to advanced language\\nunderstanding and generation capabilities. However, responsible use and ethical considerations are essential when\\nworking with these models to ensure fair and unbiased outcomes.\\n7.2 API of LLM\\nIn this section, we discuss the APIs of LLMs, which have been described in Table 9.\\nTable 9: Comparison of LLM APIs\\nAPI\\nName\\nProvider Languages\\nSupported\\nAccess\\nType\\nApplication\\nArea\\nAdvantages Constraints\\nOpenAI\\nAPI [77]\\nOpenAI Multiple lan-\\nguages\\nAPI Key NLP, text gen-\\neration, chat-\\nbots\\nState-of-the-\\nart models,\\nversatility,\\nGPT architec-\\nture\\nAPI rate\\nconstrain, cost\\nconsidera-\\ntions\\nHugging\\nFace\\nTrans-\\nformers\\n[78]\\nHugging\\nFace\\nMultiple lan-\\nguages\\nOpen\\nSource\\nNLP, model\\nfine-tuning,\\nresearch\\nLarge model\\nrepository,\\nextensive\\ncommunity\\nsupport\\nSelf-hosting\\ncomplexity,\\nno official\\nsupport\\nGoogle\\nCloud AI-\\nLanguage\\n[79]\\nGoogle\\nCloud\\nMultiple lan-\\nguages\\nAPI Key Sentiment\\nanalysis,\\nentity recogni-\\ntion, transla-\\ntion\\nGoogle’s\\nrobust infras-\\ntructure, easy\\nintegration\\nCost may vary\\nbased on us-\\nage\\nMicrosoft\\nAzure\\nLanguage\\n[80]\\nMicrosoft\\nAzure\\nMultiple lan-\\nguages\\nAPI Key Sentiment\\nanalysis,\\nentity recogni-\\ntion, language\\nunderstanding\\nIntegration\\nwith Azure\\nservices, com-\\nprehensive\\nAPIs\\nPricing based\\non usage\\nIBM Wat-\\nson NLU\\n[81]\\nIBM Wat-\\nson\\nMultiple lan-\\nguages\\nAPI Key Sentiment\\nanalysis, emo-\\ntion analysis,\\nkeyword\\nextraction\\nIBM’s AI\\nexpertise,\\ncustomization\\noptions\\nCosts may add\\nup for high us-\\nage\\nAmazon\\nCompre-\\nhend [82]\\nAmazon\\nAWS\\nMultiple lan-\\nguages\\nAPI Key Entity recog-\\nnition, senti-\\nment analysis,\\ntopic model-\\ning, document\\nclassification\\nIntegration\\nwith AWS,\\nscalability\\nCosts may\\nvary based on\\nusage\\nFacebook\\nAI’s\\nFairseq\\n[82]\\nFacebook\\nAI\\nMultiple lan-\\nguages\\nOpen\\nSource\\nNeural\\nmachine\\ntranslation,\\nlanguage\\nmodeling,\\nresearch,\\ndevelopment\\nResearch-\\noriented,\\nflexibility,\\nopen-source.\\nSelf-hosting\\nand main-\\ntenance\\ncomplexity.\\n17'), Document(metadata={'source': './data/LLMS.pdf', 'page': 17}, page_content='A Review on LLM: Architectures, Applications, Open Issues and Challenges\\nOpen AI API: The API provided by OpenAI offers access to GPT models that may be utilized for a wide range of\\ntext-related applications [83]. It facilitates many tasks such as coding, question and answer, analysis, and other related\\nactivities. The available models encompass a spectrum of options, spanning from gpt-4 to gpt-3.5-turbo, as well as\\nmany legacy variants. The Chat Completions API facilitates interactive dialogues by incorporating distinct roles such as\\nuser, and assistance. The programming language provides support for function calling, which allows for the retrieval of\\nstructured data. The OpenAI API provides developers with the capability to leverage advanced modeling of languages\\nfor a diverse range of applications.\\nHugging Face: Hugging Face provides a complimentary Inference API that facilitates the examination and assessment\\nof more than 150,000 publicly available ML models [84]. It features predictive capabilities, and integration with more\\nthan 20 open-source libraries, and facilitates fast change between models. The API facilitates a range of operations,\\nincluding classification, image segmentation, text analysis, speech recognition, and other related functionalities.\\nGoogle Cloud API: The Cloud-based NLP API developed by Google provides support for a range of approaches,\\nsuch as sentiment analysis, text analysis, entity recognition, and other text annotations [79]. The functionalities can\\nbe accessed by developers through REST API calls utilizing either the client libraries or their own custom libraries.\\nAdditionally, it offers moderation functionalities for the purpose of detecting potentially sensitive content. Several API\\nexists, and each possesses distinct features and functions.\\nMicrosoft Azure Language APIs: These APIs support many activities, including sentiment analysis, text summarization,\\nand other related tasks [80]. Developers use RESTful endpoints to include Azure LLM APIs. Microsoft provides useful\\nSDKs and code examples in other programming languages, including Python, Java, etc. to facilitate the utilization of\\nthese APIs.\\nIBM Watson Natural Language: The IBM Watson API is a robust tool for investigating and extracting valuable\\ninformation from textual data. This API offers developers a variety of functionalities, encompassing sentiment analysis,\\nemotion analysis, and additional features [81]. Due to its provision of multilingual support and a user-friendly API, this\\ntechnology enables developers to effectively include sophisticated text analytics into their programs.\\nAmazon Comprehend API: The Amazon Comprehend API is a powerful NLP service provided by Amazon Web\\nServices [82]. This tool evaluates textual data, allowing the researchers to acquire significant knowledge, such as\\nentity recognition, language detection, sentiment analysis, and topic modeling. Due to its ability to accommodate\\nmany languages and simple integration, this tool displays adaptability in addressing a range of use cases, including\\ncustomer feedback analysis and others. The utilization of this API can prove to be a significant resource for enterprises’\\nmarketing to extract practical insights from unstructured textual data.\\nFacebook AI’s Fairseq: The Fairseq framework developed by Facebook AI is a comprehensive tool for performing\\nsequence-to-sequence modeling, specifically designed for handling LLMs [85]. Fairseq is a well-suited API for many\\napplications related to analyzing and generating natural language. The platform provides support for advanced models\\nsuch as BERT and RoBERTa, allowing researchers to perform fine-tuning on these models according to specific needs.\\nIn this study, we have provided a comprehensive overview of seven popular APIs in Table 9 that leverage the capabilities\\nof LLMs for the purpose of NLP-based functionalities. However, the taxonomy revealed the presence of several other\\nAPIs that are associated with text analysis but do not utilize LLMs. The aforementioned APIs, including TextBlob,\\nTextRazor, Sapling AI, MonkeyLearn, and Aylien, etc., utilize traditional machine learning, statistical methods, and\\nrule-based natural NLP techniques instead of relying on extensive pre-trained LLMs. Since, the primary focus of\\nthis study has been on describing the tools that particularly utilize LLMs for the purpose of advanced text analysis,\\ngeneration, and comprehension, we have refrained from discussing these APIs in depth.\\n8 Domain Specific Application\\nSince there are several pre-trained models in LLMs, all of them are utilized by training or fine-tuned to perform\\nwell-defined tasks maintained by their requirements in different fields. Numerous research studies have consistently\\ncontributed by using LLMs model in diverse domains such as healthcare, finance, education, forecasting, and natural\\nlanguage processing. The extensive experiments of different LLM models contribute to revolutionizing the use of AI\\nacross these diverse domains. This section demonstrates the potential contribution of LLMs application in different\\ndomains. Table 10 illustrates the major contribution of LLMs in the specific domain, as well as outline their prospective\\nlimitations and future directions.\\nBio-Medical and Healthcare: As previously stated, GPT has several versions, ranging from GPT1 to GPT4. GPT3\\nis extremely useful in the healthcare industry since it can be trained to support customer service with no effort. It\\ncan get all required information through a conversation rather than an intake form, and many systems might be built\\n18'), Document(metadata={'source': './data/LLMS.pdf', 'page': 18}, page_content='A Review on LLM: Architectures, Applications, Open Issues and Challenges\\nTable 10: Machine learning-based study comparison in LLMs\\nDomain Author Major Contributions Limitations Future Research Direction\\nMedical\\nChen et al. [86](2023)\\nI. Assess the state-of-the-art performanceof biomedical LLMs for the purpose ofclassifying and reasoning tasks on clinicaltext data. II. Emphasizes the vulnerabilityof LLM performance in relation to promptsand addresses it.\\nI. Data limitation due to privacyconcern of biomedical data.II. Did not evaluate the performanceof the model in an out-of-domain task.\\nI. To support this study’s findings,need to experiment using real clinicaldata.II. Optimize the models to make themmore robust and resource-efficient.\\nHuang et al. [87](2023)\\nI. Investigates the possible utilization of LLMs,specifically ChatGPT and its variety within thedomain of dentistry.II. Design a MultiModal LLM system forclinical dentistry application and addresscritical challenges to revolutionizedental diagnosis.\\nI. Lack of data resulted in thepost-training process, raising concernsabout the model’s reliability.II. The possibility of data breaches hasno strict security method.III. Requires intensive computationalcost.\\nI. Reducing operational costs by fine-tuningthe model and enhancing efficiency.II. Explore diverse medical data to providepersonalized dental care.\\nSorin et al. [88](2023)\\nI. Evaluating the efficacy of ChatGPT-3.5 asa supporting tool for facilitating clinicaldecision-making in breast tumor cases.II. Outlines the implementation of a gradingsystem for evaluating the responses generatedby ChatGPT.\\nI. Conducting the experiment with a smallsample size leads to performance bias inthe model.II. Human errors in the grading system canpotentially add biases to the system.\\nI. More diverse sample of breast tumor casesto increase ChatGPT’s performance andgeneralizability.II. Introducing a multimodal approach toincrease the reliability of clinicalrecommendations.\\nThirunavukarasu et al.[89] (2023)\\nI. Focuses on the energy and environmentalimpact of training LLM models such asGPT-3 and GPT-4 and emphasize costreduction to make them more accessible.II. Examines the utilization of LLMs modelsin the medical domain, specifically focusingon medical education and medical research.\\nI. Inaccuracies observed in the responsesprovided to queries due to the lack ofupdates on the training data.II. Lack of interpretability of LLMs modelsince it is a black box, hence the concept wasfrequently misunderstood.\\nI. Emphasis on integrating more recent andup-to-date training data.II. Further investigation should strive toenhance the transparency and interpretabilityof LLMs.III. Including the feasibility of implementingrandomized trials to evaluate the effects ofLLM on medical outcomes.\\nKorngiebel et al. [90](2021)\\nI. Discuss the benefits and potential pitfalls ofNLP technologies in eHealth.II. Discuss the benefits of using GPT in themedical domain.\\nI. Conversational AI like GPT-3 will notreplace human interaction in healthcaresoon, despite extensive development.II. Examines GPT’s applicability in acertain medical domain.\\nI. Analyze GPT’s impact on real-worldhealthcare settings to assess its performance.II. Provide personalized healthcare byanalyzing a variety of medical data.\\nAngelis et al. [91](2023)\\nI. examine LLMs’ ethical and practical issues,focusing on medicinal use and public health.II. Discuss how ChatGPT can provide false ormisleading information.III. Suggest the detectable-by-design techniqueto spot fake news or information.\\nI. The addition of a detectable-by-designthe technique may slow LLM developmentand AI business acceptance.II. Experimental data has been limited dueto medical data privacy concerns.\\nI. An experiment using real clinical data isneeded to support the findings.II. Further research should be conducted tospeed up the entire procedure.\\nSallam et al. [92](2023)\\nI. Saves time in scientific research throughcode delivery and literature review.II. Makes the publication process faster byproviding better research ideas and results.III. Reduces potential costs and increasesefficiency in healthcare delivery.IV . Enhances communication skills inhealthcare education through properacademic mentoring.\\nI. Copyright issues, bias based on the trainingdataset, plagiarism, over-detailed content, lackof scientific accuracy, limited updatedknowledge, and lack of ability to criticallydiscuss the results in using ChatGPT in scientificresearch.II. Unable to understand the complexityof biological systems, lack of emotional andpersonal perspective,inaccurate content, bias, and transparency issuesin healthcare practice.III. Copyright issues, inaccurate references,limited updated knowledge, and plagiarism inhealthcare education.\\nI. Accountability, honesty, transparency, andintegrity must be considered in scientificresearch.\\nII. To enhance healthcare and academics,ChatGPT should uphold ethical principles.Potential dangers and other issues must alsobe considered.\\nIII. An AI editor and an AI reviewer inacademic writing to advance academicresearch, given the previous shortcomingsof the editorial and peer review process.\\nCascella et al. [93](2023) I. Support of clinical practiceII. Scientific writing\\nI. Generates answers that sound plausible butmay be incorrect or meaningless and biasedbased on trained data.\\nI. Enhance the ability to answer medicalquestions and provide the context forunderstanding complex relationshipsbetween various medical conditionsand treatments.\\nKung et al. [94](2023)\\nI. The investigation of AI within the contextof medical education.II. Assessment of ChatGPT’s Performance inClinical Decision-making.III. Explore the demands of AI in medicaleducation to standardize methods and readoutsand quantify human-AI interactions\\nI. The experiment is conducted on a smallinput size.II. Human adjudication variability and bias.III. The absence of real-life instructionalscenarios.\\nI. To evaluate the efficacy of ChatGpt inreal-world clinical practice by assessingits performance and impact.II. A comprehensive analysis of ChatGPT’seffectiveness in relation to subject taxonomy.\\nGu et al. [95](2021)\\nI. Shows that domain-specific pretraining fromscratch outperforms mixed-domain inbiomedical NLP.II. Formulate a new dataset using theBiomedical set of diverse tasks.\\nI. Explore the applicability only in a fixedBiomedical Domain.II. Future modifications of the benchmark maybe required to reflect the effectiveness of theresearch.\\nI. An Investigation and analysis intopretraining strategies.II. The addition of Biomedical NLP tasks.III. Exploring other domains for comparativeanalysis.\\nKraljevic et al. [96](2022)\\nI. Introduced a foresight application based onelectronic health records.II. Develop a multifunctional model.III. Conduct experiments in different hospitals.\\nI. Should include metrics, and comparativeanalysis in real-world clinical scenarios toevaluate Foresight’s performance.II. Integrate enough security on health recordsto protect the privacy of the patients.\\nI. Integrating input from healthcare specialistsand consistently updating the model with thelatest medical data.II. Implement a real-life scenario to investigatethe clinical application of Foresight.\\nTourism\\nMich et al. [97](2023)\\nI. Highlights how ChatGPT is contributing tothe tourism sector by identifying new targetmarkets, implementing the marketing strategydesigns, and improving customer service.\\nI. Transparency and accountability issues: thedataset is not updated, and can not see the logicof what is wrong and what is right.\\nI. Applications should increase user trust andfact-checking.\\n19'), Document(metadata={'source': './data/LLMS.pdf', 'page': 19}, page_content='A Review on LLM: Architectures, Applications, Open Issues and Challenges\\nTable 10: (Continued) Machine learning-based study comparison in LLMsDomain Author Major Contributions Limitations Future Research Direction\\nIndustry\\nYu et al. [98](2023)\\nI. Examines how LLMs can use their superiorknowledge and reasoning to predict financialtime series.II. Focuses on NASDAQ-100 stocks usingpublicly available historical stock price data.III. To prove LLMs can solve problemscomprehensively, experiments are conducted.\\nI. The study utilizes a small amount of datasamples.II. Data is collected from only one specificdomain.III. Utilizing a small sample size duringexperiments cause performance bias.\\nI. SP500 and Russell 2000 stock indexeswill be added to the research.II. The research will use macro-economytime series, stock trading volumes, and socialnetwork data.III. To improve reasoning, larger public modelslike 30B will be refined.\\nFrederico et al. [99](2023)\\nI. Discusses the uses and concerns with ChatGPTin supply chains.II.Provide supply chain specialistsadvice about ChatGPT’s effects andusage.\\nI.A limited amount of data is used in theexperiment.II. Did not assess the efficacy of ChatGPTin practical industrial settings.\\nI. Analyze how ChatGPT can enhance thesupply chain efficiency.II. Discuss supply chain ChatGPTimplementationissues and success factors.\\nGaming\\nSobieszek et al. [100](2022)\\nI. Examines the efficacy of employing LLMas a gaming tool.II. Assess the performance of GPT in thecontext of the Turing test.III. Analyze the boundaries of LLMs.IV . Discuss the challenges these modelsencounter in accurately conveyinginformation.\\nI. They did not employ a well-curated set oftargeted questions.II. It may produce answers that are eithererroneous or lack significance.\\nI. Assess the performance of LLM byadministering inquiries across diversedomains.\\nEducation\\nAbramski et al. [42](2023)\\nI. Utilized network science and cognitivepsychology to study biases toward math andSTEM across language models.II. Behavioral Forma Mentis Networks(BFMNs) are used to understand how LLMscomprehend arithmetic, STEM, and similarconcepts.\\nI. Commercial GPT systems can be testedby researchers but not replicated by everyonedue to their structure.II. The old interface or API system no longerallows public access to GPT-3.\\nI. Putting a priority on integrating data fromtraining that is up-to-date.II. Investigating several other fields for thepurpose of comparative research.III. More information from students atdifferent institutions will be gathered.\\nKasneci et al. [20](2023)\\nI.Helps students develop critical thinking inreading and writing, provides practice problemsand quizzes, helps improve research skills, andimproves various developmental skills.II.Provides guidance to teachers on how to improvestudent learning in each aspectof teaching and helps develop teaching materials.\\nI. Helpful only for English-speaking people,but also for people of other languages cannotenjoy the benefits.II.Consumes high energy and financialcost of maintenance.III. Negative effect on critical thinking andproblem-solving skills of students and teachers.IV .Privacy and security risks to students’personal and sensitive information.\\nI. Creating an age-appropriate user interfacethat maximizes the benefits and minimizesthe pitfalls of interaction with AI-based tools.II. To guarantee equity for all educational entitiesinterested in current technologies, governmentorganizations may regulate financial obstacles toaccessing, training, and maintaining largelanguage models.\\nHadi et al. [101](2023)\\nI. Helps students save labor and time by assigningassignments and helps teachers automate the gradingprocess, and provides detailed feedback to students,which reduces their workload.II.Aid decision-making, problem-solving and promotelearning in medical education.III.Provides financial advice based on their queries toimprove customer service, and provides various stepsbased on financial algorithms to reduce risk byanalyzing past market data.IV . Saves software engineers time and increasesoverall efficiency by providing code snippets,identifying and generating test cases, etc.\\nI. Bias, reasoning errors, counting errors,information hallucination, LLMs explainability.\\nI. Improving the accuracy and performanceof LLMs, addressing their limitations, andexploring new ways to utilize them.\\nLo et al. [102](2023) I. Helps students in learning and assessment and helpsteachers in teaching preparation and assessment.\\nI. Negative effect on critical thinking andproblem-solving skills of students andteachers.\\nI. Training instructors on how to effectivelyuse ChatGPT and identify student intelligence.Also, educate students about the uses andlimitations of ChatGPT.\\nDwivedi et al. [103](2023)\\nI. Highlights the challenges, opportunities, and impacts ofChatGPT in education, business, and society, as well asinvestigates important research questions asked ofChatGPT across the education, business, and society sectors.\\nI. The generated text is hard to understandand can’t answer questions correctly unlessphrased a certain way, lacks updated information,and doesn’t automatically update the actual data.\\nI. Teaching, learning, and scholarly research,digital transformation organization and society,knowledge, transparency, and ethics to enhanceChatGPT’s efficiency in all these areas.\\nto assist numerous patients at the same time [ 90]. Besides, clinics and hospitals are places to cure illness, but it is\\nalso true that various contagious viruses are brought into these places. Patients and healthcare providers can be better\\nprotected from infection by replacing a human receptionist with a robot. This becomes increasingly important during the\\nCOVID-19 epidemic [104]. Since clinics and hospitals often see a high volume of patients on a daily basis, an optimum\\nand lightweight system may submit several queries for single patients to create acceptable output. Consequently,\\nGPT models can also aid in cost reduction in the medical industry. Furthermore, biomedical and clinical text mining\\nhas always been an essential and major challenge due to the complex nature of domain corpora and the continually\\nexpanding number of documents. As a result, using the BERT models improves the performance of biomedical and\\nclinical text mining models [105]. Salam et al. [92] and Korngiebel et al. [90] demonstrate the substantial advantages\\nof ChatGPT in the domains of healthcare, clinical research, and practice, although simultaneously underscoring the\\nimperative necessity for proactive inspection and ethical transparency. Several studies [93, 95, 96, 89] investigations at\\nexploring the prospective utilities and constraints of LLMs such as ChatGPT within the healthcare domain, namely in\\nthe context of clinical practice, research, and public health. In their study, Kung et al. [94] conducted an evaluation of\\nChatGPT’s performance on the United States Medical Licensing Examination (USMLE), and the outcomes indicate\\nthe potentiality of LLMs to support clinical decision-making and medical education. Sorin et al. [ 88] evaluated\\nChatGPT-3.5 as a decision support for breast tumor boards where they compared the tumor board’s explanations, and\\nsummaries with ChatGPT-3.5 and showed that ChatGPT-3.5 and the tumor board had a high degree of decisional\\nalignment. Huang et al. [ 87] (year) investigate the prospective applications of LLMs with a specific emphasis on\\nChatGPT, in the field of dentistry, mainly focusing on automated dental diagnosis and highlighting the efficacy of\\nLLMs in dental diagnosis. Furthermore, the XLNet contributes to better clinical note representation by adding temporal\\ninformation and a realistic prediction setup [106]. Furthermore, various LLM models also assist this medical industry\\nby making the procedure easier than previously.\\n20'), Document(metadata={'source': './data/LLMS.pdf', 'page': 20}, page_content='A Review on LLM: Architectures, Applications, Open Issues and Challenges\\nEducation: Educators have long struggled with unequal educational resources to student demand across disciplines.\\nOne of the significant challenges is a shortage of accessible educational resources for pupils to study outside of school.\\nAlthough online instructional videos are helping to alleviate the problem, society still hopes that AI will deliver\\nindividualized teaching services to satisfy the learning demands of each student and increase teaching efficiency. So,\\nthe LLM models are very significant and have the potential to revolutionize many facets of learning, teaching, and\\neducational research in the education sector [104]. So, the GPT model aids the students in converting the math word\\nproblems into representative equations [107]. Kasenci et al. [20] highlighted substantial impact of LLMs in education\\nby facilitating personalized learning, automating grading process, and accessibility of educational resources. Hadi et\\nal. [101] presents a thorough analysis of LLMs, covering their historical development, wide-ranging applications in\\ndomains such as medicine, engineering, education, and their potential impact on the trajectory of AI. Lo et al., [102]\\nand Dwivedi et. al. [103] investigate the prospective uses of ChatGpt within the realm of education and identify the\\nprimary obstacles that have arisen during its initial deployment. Besides, in terms of writing authentic texts in distinct\\nformats, including essays, summaries, and articles, these models help to accomplish this without any error. In contrast,\\nthe manual process may have human errors in the documentation. In this case, the GPT model helps to address this\\nproblem. In addition, the XLNet Excel method also helps understand the texts and documents that can be employed\\nin the education sector [39]. Furthermore, other models significantly impact the education system, making it more\\nengaging, accessible, and productive for both students and teachers.\\nSocial Media: The LLMs have revolutionized several aspects of the social media industry regarding content production,\\nmoderation, sentiment analysis, etc. There are some crucial aspects of the LLMs in the social media sector in terms of\\nwriting content, generating images, classifying and generating text, and even full blogs and articles for social media.\\nAlso, these models can perform named entity recognition (NER) and text classification [ 108, 109]. When the GPT,\\nXLNet, BERT, etc., model aids the writer and content producers in generating a consistent flow of excellent material.\\nIt also provides content suggestions, and to create a safer online environment, these models are hired to assist in\\ndiscovering and filtering out different dangerous and improper content. In their study, Abramski et al. [ 42] utilized\\nnetwork science and the principles of cognitive psychology to evaluate biases present in LLMs. Sobieszek et al. [100]\\npresents a critical examination of the stated semantic capabilities of GPT-3, aiming to challenge the current view of its\\ndismissal. Moreover, it assists in determining public opinion on certain topics by analyzing public interest and demand.\\nBusiness: In business, LLM helps companies improve their decision-making processes, product manufacturing\\nprocesses, operations, and customer interactions. Communicating with customers and providing 24/7 customer service\\nby answering their queries, assisting them in their work, and providing advanced advice related to areas of interest\\nto customers is crucial for business progress. Moreover, it is also important to analyze customer sentiment, market\\ntrends, risk factors, and competitive intelligence [?]. In this case, LLMs help to fulfill all their requirements within a\\nshort period. The LLM models, like GPT, XLNet, BERT, etc., play a vital role in creating customer documents and\\nproduct details and efficiently maintaining the entire business by saving time and reducing laborious tasks. Frederico et\\nal. [99] presents an initial investigation into the potential applications and effects of ChatGPT in the domain of supply\\nchain management. Their study provides significant insights for professionals engaged in this domain. Mich et. al.\\n[97] present an initial investigation of potential hazards associated with the implementation of ChatGPT in bussiness\\ndomain. Yu et al. [98] presented an analysis of the capabilities of LLMs, specifically GPT-4, in the context of financial\\nforecasting for a time series. Besides, their findings reveal that the performance of LLMs outperforms other traditional\\nmodels also.\\nAgriculture: In agriculture, variations of GPT models, including GPT3, BERT, and XLNet models, play a significant role\\n[110, 111]. They are able to analyze large data hubs of soil, crop, and weather data along with satellite imagery. They\\ncan provide recommendations on plating times, irrigation, fertilizer application, and optimizing fields and resources.\\nFarmers can obtain current updates and market requirements, predict crop prices, anticipate natural disasters, and\\ndocument farmers’ and crop details. Manual agriculture management can be time-consuming and laborious, but these\\nmodels can handle all the issues.\\n9 Impact of Large Language Models on Society\\nLarge Language Models (LLMs) and similar AI technologies have had a profound impact on society across various\\ndomains. While these technologies offer many benefits, they also raise important ethical, social, and economic\\nconsiderations. Here’s an overview of the impact of LLMs on society:\\na. Advancements in Natural Language Processing (NLP): LLMs have significantly advanced the field of NLP, making\\nit possible to automate and scale a wide range of language-related tasks such as translation, summarization, sentiment\\nanalysis, and more. In recent years, Natural Language Processing (NLP) has witnessed significant advancements,\\nprimarily driven by the emergence of Large Language Models (LLMs). These advancements, exemplified by models\\n21'), Document(metadata={'source': './data/LLMS.pdf', 'page': 21}, page_content='A Review on LLM: Architectures, Applications, Open Issues and Challenges\\nFigure 7: Visual representation of impact on LLMs\\nsuch as BERT [11], RoBERTa [46], and XLNet [73], have transformed the NLP landscape. Notably, LLMs have been\\nfine-tuned for various specific NLP tasks, enabling remarkable performance improvements. Multilingual models like\\nmBERT [112] and cross-lingual models like XLM-R [ 113] have facilitated language understanding across diverse\\nlinguistic contexts. Additionally, there has been a focus on creating more efficient versions of LLMs such as DistilBERT\\n[114] and ALBERT [ 115]. These developments have not only expanded the applicability of NLP but have also\\nraised ethical considerations, prompting research in bias mitigation [ 116] and responsible AI. LLMs have enabled\\nbreakthroughs in applications like conversational AI, few-shot and zero-shot learning, and domain-specific NLP in fields\\nlike healthcare and finance. These advancements underscore the pivotal role of LLMs in advancing the capabilities of\\nNLP and continue to shape the future of language understanding and generation.\\nb. Automation and Efficiency: LLMs are used to automate tasks that were previously time-consuming and labor-\\nintensive, leading to increased efficiency in industries such as customer support, content generation, and data analysis.\\nThe automation and efficiency of Large Language Models (LLMs), driven by models like BERT and GPT, have\\nrevolutionized industries and applications. These models have automated intricate language-related tasks, from\\nsentiment analysis to language translation, making them more efficient and accessible. LLMs, such as DialoGPT\\n[117] and ChatGPT, have powered conversational AI, streamlining customer support and interactions. Moreover, they\\nexcel in few-shot and zero-shot learning, as demonstrated by GPT-3 [118], automating tasks with minimal examples.\\nMultilingual LLMs like mBERT have automated language tasks across various languages, enhancing global accessibility.\\nEfficiency has further advanced through models like DistilBERT and ALBERT, which maintain performance while\\nreducing computational resources. These models can be fine-tuned for specific domains, such as healthcare [ 119],\\nmaking them indispensable in automating domain-specific tasks efficiently.\\nc. Content Generation: LLMs are capable of generating human-like text, which has implications for content creation,\\nincluding automated news articles, marketing materials, and creative writing.\\nd. Language Translation: LLMs have improved machine translation systems, making communication across languages\\nmore accessible and accurate.\\ne. Virtual Assistants and Chatbots: LLMs power virtual assistants and chatbots, enhancing customer service and\\nproviding round-the-clock support in various industries.\\nf. Medical and Scientific Research: LLMs are used to analyze and summarize vast amounts of medical and scientific\\nliterature, aiding researchers in finding relevant information quickly.\\ng. Accessibility: LLMs have the potential to improve accessibility by providing real-time translation and transcription\\nservices for individuals with hearing impairments or language barriers.\\nh. Personalization: LLMs enable personalized recommendations and content curation on platforms such as social\\nmedia, e-commerce, and news websites.\\ni. Creative Tools: LLMs are used as creative tools in various art forms, including generating poetry, music, and visual\\nart.\\nj. Ethical Concerns: Bias and fairness issues in LLMs have raised ethical concerns. LLMs may perpetuate or amplify\\nbiases present in training data, leading to unfair or discriminatory outcomes.\\n22'), Document(metadata={'source': './data/LLMS.pdf', 'page': 22}, page_content='A Review on LLM: Architectures, Applications, Open Issues and Challenges\\nk. Misinformation and Disinformation: LLMs can generate realistic-sounding fake text, raising concerns about the\\nspread of misinformation and disinformation.\\nl. Job Displacement: The automation capabilities of LLMs may lead to job displacement in certain industries,\\nparticularly in routine data-entry and content-generation roles.\\nm. Data Privacy: The use of LLMs often involves processing large amounts of user-generated text data, which raises\\ndata privacy concerns, especially regarding sensitive or personal information.\\nn. Economic Impact: The adoption of LLMs can disrupt traditional business models and create economic shifts as\\nindustries adapt to automation and AI technologies.\\no. Regulation and Accountability: Policymakers and regulators are grappling with the need to establish guidelines and\\nregulations for the responsible use of LLMs, including addressing issues of bias, transparency, and accountability.\\np. Education and Skill Development: The rise of LLMs underscores the importance of education and skill development\\nin AI and data science, as these technologies become increasingly integral to various industries.\\nThe impact of LLMs on society is multifaceted, and it is important to consider both the positive and negative conse-\\nquences. As these technologies continue to evolve, stakeholders, including governments, businesses, researchers, and\\nthe general public, must work together to harness the benefits of LLMs while addressing their challenges and ethical\\nimplications. The visual representation of Figure 7 effectively demonstrates the impact of LLMs, outlining their benefits\\non the left and the adversarial impacts on the right side. The utilization of this figure will provide a distinct and easily\\nunderstandable visual depiction of LLMs’ impact across different domains.\\n10 Open issues, Challenges, Future works\\nThis section discusses critical analysis of open issues, challenges, and LLMs’ future scope.\\n10.1 Open Issues\\nIn this section, we delve into the critical open issues surrounding LLMs. These concerns are at the vanguard of artificial\\nintelligence research and development. They emphasize the need for ongoing research and innovation to resolve issues\\nthat have emerged alongside the rapid development of LLMs. Our discussion will cast light on the significance of these\\nunresolved issues, highlighting their impact on various applications and the AI landscape as a whole.\\n• Issue 1: Ethical and Responsible AI\\nThe question regarding how to ensure the ethical use of large language models remains unresolved. Filtering,\\nmoderation, and accountability concerns regarding AI-generated content remain troublesome. Misinformation,\\nhate speech, and biased content generated by LLMs necessitate continuous research and development [120].\\n• Issue 2: Multimodal Integration\\nWhile LLMs are predominantly concerned with text, there is a growing demand for multimodal models that\\ncan comprehend and generate content that includes text, images, and other media types [ 121]. Integrating\\nmultiple modalities into a single model poses difficulties in data acquisition, training, and evaluation.\\n• Issue 3: Energy Efficiency\\nThe environmental impact of training and deploying large language models is still an urgent concern [122]. It\\nis essential to develop more energy-efficient training methods, model architectures, and hardware solutions to\\nreduce the carbon footprint of LLMs.\\n• Issue 4: Security and Adversarial Attacks\\nLLMs are vulnerable to adversarial assaults, where slight input modifications can lead to unexpected and\\npotentially harmful outputs [123]. Improving model robustness and security against such assaults is a crucial\\narea of study, particularly for cybersecurity and content moderation applications.\\n• Issue 5: Privacy and Data Protection\\nAs LLMs become more competent, user privacy and data protection concerns increase. Finding methods for\\nusers to interact with these models without compromising their personal information is an ongoing challenge.\\nThere is a need for research on privacy-preserving techniques and regulatory compliance [124].\\n• Issue 6: Generalization and Few-Shot Learning\\nLLMs excel when there is abundant data but struggle with tasks requiring few examples or domain-specific\\nknowledge. Improving their capacity to generalize and perform well with limited training data is a crucial area\\nof research [125].\\n23'), Document(metadata={'source': './data/LLMS.pdf', 'page': 23}, page_content='A Review on LLM: Architectures, Applications, Open Issues and Challenges\\n• Issue 7: Cross-Lingual and Low-Resource Settings\\nIt is an ongoing challenge to make LLMs more accessible and effective in languages and regions with limited\\nresources and data [126]. Global applications require developing techniques for cross-lingual transfer learning\\nand low-resource language support.\\n10.2 Challenges\\nLLMs have rapidly evolved from being non-existent to becoming a ubiquitous presence in the field of machine learning\\nwithin just a few years. Their extraordinary ability to generate text that resembles that of a human has garnered\\nsignificant attention and applications in numerous fields. However, this meteoric rise in prominence has also revealed\\nmany challenges and concerns that must be addressed to realize the potentiality of these models fully. In this discussion,\\nwe will examine ten of the most significant challenges pertaining to LLMs.\\n• Challenge 1: Data Complexity and Scale\\nIn the era of LLMs, the size and complexity of the datasets on which they are trained is one of the most\\nsignificant challenges. These models are typically trained on enormous corpora of Internet-sourced text data.\\nThese datasets are so extensive that it is nearly impossible to comprehend or investigate the totality of their\\ninformation. This raises concerns regarding the quality and biases of the training data and the potential for the\\nunintentional dissemination of detrimental or inaccurate information.\\n• Challenge 2: Tokenization Sensitivity\\nFor analysis, LLMs rely significantly on tokenization, dividing text into smaller units (tokens) [127]. Tokeniza-\\ntion is essential for language processing and comprehension but can also present challenges. For instance, the\\nmeaning of a sentence can alter significantly based on the choice of tokens or the ordering of words. This\\nsensitivity to input phrasing can lead to unintended outcomes when generating text, such as adversarial assaults\\nand output variations based on minute input changes.\\n• Challenge 3: Computational Resource Demands\\nThe training of LLMs is a computationally intensive procedure that requires substantial hardware and energy\\nresources. It is necessary to have access to supercomputing clusters or specialized hardware in order to train\\nlarge models, and the environmental impact of such resource-intensive training has raised concerns. Significant\\nenergy consumption is associated with training LLMs at scale, contributing to the AI industry’s overall carbon\\nfootprint.\\n• Challenge 4: Fine-Tuning Complexity\\nWhile pre-training gives LLMs a broad comprehension of language, fine-tuning is required to adapt these\\nmodels to specific tasks. Fine-tuning entails training the model on a smaller dataset, frequently requiring\\nhuman annotators to label examples. As it involves the construction of task-specific datasets and extensive\\nhuman intervention, this process can be both time-consuming and costly.\\n• Challenge 5: Real-Time Responsiveness\\nThe remarkable training capabilities of LLMs come at the expense of inference speed. Real-time response or\\nprediction generation with these models can be sluggish, limiting their applicability in applications such as\\nchatbots or recommendation systems where low-latency responses are crucial for user satisfaction.\\n• Challenge 6: Contextual Constraints\\nLLMs can only evaluate a limited number of preceding tokens when generating text due to their limited\\ncontext window. This limitation presents difficulties when working with lengthy documents or having lengthy\\nconversations. Maintaining coherence and relevance over lengthy text sequences can be challenging because\\nthe model may neglect or lose track of pertinent information.\\n• Challenge 7: Bias and Undesirable Output\\nIn their output, LLMs can display biases or undesirable characteristics. This is due to the inherent biases in the\\ntraining data, which are assimilated by the model and reflected in its responses. Such biases can manifest as\\nobjectionable, discriminatory, or harmful content, making it imperative to address and mitigate these concerns\\nto ensure the responsible deployment of AI.\\n• Challenge 8: Knowledge Temporality\\nLLMs learn using historical data from the Internet, and their knowledge is restricted to what is available as of\\na particular date. Consequently, they may lack access to the most recent information or events. This can be\\nproblematic when users expect up-to-date responses or when the conversation involves recent events.\\n24'), Document(metadata={'source': './data/LLMS.pdf', 'page': 24}, page_content='A Review on LLM: Architectures, Applications, Open Issues and Challenges\\n• Challenge 9: Evaluation Complexity\\nEvaluation of LLMs presents significant difficulties. Many extant evaluation metrics are insufficient to capture\\nthe nuances of model performance, which raises questions about their efficacy. Additionally, these metrics can\\nbe susceptible to manipulation or gaming, which may provide an inaccurate image of a model’s capabilities. To\\nassess LLMs’ actual performance and limitations, robust and reliable evaluation methodologies are required.\\n• Challenge 10: Dynamic Evaluation Needs\\nFrequently, evaluating LLMs entails comparing their outputs to static benchmarks or human-authored ground\\ntruth. However, language is dynamic and evolves, and preset evaluation data may not adequately reflect a\\nmodel’s adaptability to language and context change. This difficulty underscores the need for evaluation\\nframeworks that are more dynamic and continually updated.\\n10.3 Future Works\\nEmerging in the rapidly evolving landscape of LLMs are several key research foci and directions that will shape\\nthe future of these robust AI systems. Improving Bias Mitigation involves refining training data to minimize bias,\\ndeveloping effective debiasing techniques, establishing guidelines for responsible AI development, and integrating\\ncontinuous monitoring and auditing mechanisms into AI pipelines to guarantee fairness and impartiality.\\nAnother essential concern is efficiency, which has prompted research into more efficient training techniques. This\\nincludes exploring innovative techniques such as federated learning to distribute training across decentralized data\\nsources, investigating knowledge distillation methods for model compression, and discovering ways to reduce the\\nsubstantial computational and environmental costs associated with LLMs. Dynamic Context Handling is crucial for\\nenhancing the capabilities of LLMs. This involves enhancing their context management so that they can comprehend\\nlengthier context windows and handle lengthy documents or conversations with greater ease. These enhancements\\ncan substantially increase their usefulness in a variety of applications. To maintain LLMs relevant and up-to-date, it\\nis essential to enable continuous learning. This involves developing techniques that enable these models to adapt to\\nevolving language and knowledge over time, ensuring that they remain valuable and accurate sources of information.\\nMoreover, interpretable AI is an absolute necessity. This requires the development of methods to make LLM outputs\\nmore transparent and interpretable, thereby nurturing confidence and comprehension in AI decision-making processes.\\nThe development of multimodal LLMs that incorporate text, vision, and other modalities is an intriguing frontier.\\nThese models can comprehend and generate text from images, videos, and audio, creating new opportunities for AI\\napplications in various fields. Collaboration between humans and artificial intelligence is also a crucial focal area.\\nResearch on how humans and LLMs can collaborate effectively, with AI assisting and augmenting human tasks, will\\nbe crucial for developing advanced AI applications in various fields. There is a need for dynamic evaluation metrics\\nthat can adapt to changing language and context in the context of evaluation. Developing relevant and up-to-date\\nbenchmarks is essential for accurately assessing LLM performance. Personalization and customization are becoming\\nincreasingly important for boosting user contentment. Exploring techniques to customize LLM interactions to the\\npreferences and needs of individual users can considerably enhance their utility in a variety of applications.\\nLastly, as AI regulation evolves, it’s vital to work on developing ethical and legal regulatory frameworks that guide the\\nresponsible use of LLMs and ensure compliance with data protection and privacy regulations. These frameworks will\\nplay a pivotal role in regulating LLMs’ ethical and responsible deployment in society. In conclusion, these research\\ndirections collectively pave the way toward maximizing the potential of LLMs while ensuring their accountable and\\nethical use in our evolving AI landscape.\\n11 Conclusion\\nThe field of LLMs has witnessed a remarkable evolution and expansion, resulting in extraordinary capabilities in natural\\nlanguage processing (NLP) and various applications in various areas. Based on neural networks and the transformative\\ntransformer architecture, these LLMs have revolutionized our approach to machine language comprehension and\\ngeneration. The thorough review of this research has provided an insightful overview of LLMs, encompassing their\\nhistorical development, architectural foundations, training methods, and vast advancement resources. It has also\\nexamined the various applications of LLMs in disciplines such as healthcare, education, social sciences, business, and\\nagriculture, demonstrating their potential to address real-world issues. In addition, this review has delved into the\\nsocietal effects of LLMs, discussing how they shape the future of AI and can be utilized to address complex problems.\\nHowever, it has not shied away from addressing the pressing challenges and ethical considerations associated with\\ndeploying LLMs, including model biases, privacy concerns, and the need for enhanced robustness and controllability.\\nAs the field of LLM research continues to evolve swiftly, this review is a valuable resource for practitioners, researchers,\\nand experts seeking a comprehensive understanding of LLMs’ past, present, and future. It emphasizes the significance\\n25'), Document(metadata={'source': './data/LLMS.pdf', 'page': 25}, page_content='A Review on LLM: Architectures, Applications, Open Issues and Challenges\\nof ongoing efforts to improve the efficacy and dependability of LLMs, as well as the need for ethical development\\nand deployment practices. LLMs represent a pivotal advancement in AI and NLP, with the potential to revolutionize a\\nvariety of domains and solve complex problems. This article provides a comprehensive foundation for future research\\nand development in Large Language Models’ dynamic and thrilling field.\\nReferences\\n[1] Steven Pinker. The language instinct: How the mind creates language. Penguin uK, 2003.\\n[2] Marc D Hauser, Noam Chomsky, and W Tecumseh Fitch. The faculty of language: what is it, who has it, and\\nhow did it evolve? science, 298(5598):1569–1579, 2002.\\n[3] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang,\\nJunjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.\\n[4] IBY AM Turing. Computing machinery and intelligence-am turing. Mind, 59(236):433, 2007.\\n[5] Yiqiu Shen, Laura Heacock, Jonathan Elias, Keith D Hentel, Beatriu Reig, George Shih, and Linda Moy. Chatgpt\\nand other large language models are double-edged swords, 2023.\\n[6] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780,\\n1997.\\n[7] Mengnan Du, Fengxiang He, Na Zou, Dacheng Tao, and Xia Hu. Shortcut learning of large language models in\\nnatural language understanding: A survey. arXiv preprint arXiv:2208.11857, 2022.\\n[8] Bhuvana Ramabhadran, Sanjeev Khudanpur, and Ebru Arisoy. Proceedings of the naacl-hlt 2012 workshop:\\nWill we ever really replace the n-gram model? on the future of language modeling for hlt. In Proceedings of\\nthe NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language\\nModeling for HLT, 2012.\\n[9] Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. Recurrent neural network\\nbased language model. In Interspeech, volume 2, pages 1045–1048. Makuhari, 2010.\\n[10] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\\nand Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\n[12] Yash Khare, Viraj Bagal, Minesh Mathew, Adithi Devi, U Deva Priyakumar, and CV Jawahar. Mmbert:\\nMultimodal bert pretraining for improved medical vqa. In 2021 IEEE 18th International Symposium on\\nBiomedical Imaging (ISBI), pages 1033–1036. IEEE, 2021.\\n[13] Ruibo Liu, Chenyan Jia, Jason Wei, Guangxuan Xu, Lili Wang, and Soroush V osoughi. Mitigating political\\nbias in language models through reinforced calibration. In Proceedings of the AAAI Conference on Artificial\\nIntelligence, volume 35, pages 14857–14866, 2021.\\n[14] Katharine Sanderson. Gpt-4 is here: what scientists think. Nature, 615(7954):773, 2023.\\n[15] Sundar Pichai. An important next step on our ai journey, feb 2023. URL https://blog. google/technology/ai/bard-\\ngoogle-ai-search-updates, 2(9), 2023.\\n[16] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and\\nTatsunori B Hashimoto. Alpaca: A strong, replicable instruction-following model. Stanford Center for Research\\non Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html, 3(6):7, 2023.\\n[17] Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey.arXiv preprint\\narXiv:2212.10403, 2022.\\n[18] Lizhou Fan, Lingyao Li, Zihui Ma, Sanggyu Lee, Huizi Yu, and Libby Hemphill. A bibliometric review of large\\nlanguage models research from 2017 to 2023. arXiv preprint arXiv:2304.02020, 2023.\\n[19] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang\\nWang, Yidong Wang, et al. A survey on evaluation of large language models. arXiv preprint arXiv:2307.03109,\\n2023.\\n[20] Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs\\nGasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, et al. Chatgpt for good? on opportunities and\\nchallenges of large language models for education. Learning and individual differences, 103:102274, 2023.\\n26'), Document(metadata={'source': './data/LLMS.pdf', 'page': 26}, page_content='A Review on LLM: Architectures, Applications, Open Issues and Challenges\\n[21] Muhammad Usman Hadi, R Qureshi, A Shah, M Irfan, A Zafar, MB Shaikh, N Akhtar, J Wu, and S Mirjalili. A\\nsurvey on large language models: Applications, challenges, limitations, and practical usage. TechRxiv, 2023.\\n[22] Marko Kardum. Rudolf carnap–the grandfather of artificial neural networks: The influence of carnap’s philosophy\\non walter pitts. Guide to Deep Learning Basics: Logical, Historical and Philosophical Perspectives , pages\\n55–66, 2020.\\n[23] Geoffrey Leech. Corpora and theories of linguistic performance. Svartvik, J. Directions in Corpus Linguistics,\\npages 105–22, 1992.\\n[24] Elizabeth D Liddy. Natural language processing. 2001.\\n[25] Blaise Cronin. Annual review of information science and technology. 2004.\\n[26] Daniel S Hain, Roman Jurowetzki, Tobias Buchmann, and Patrick Wolf. A text-embedding-based approach to\\nmeasuring patent-to-patent technological similarity. Technological Forecasting and Social Change, 177:121559,\\n2022.\\n[27] Georgina Curto, Mario Fernando Jojoa Acosta, Flavio Comim, and Begoña Garcia-Zapirain. Are ai systems\\nbiased against the poor? a machine learning analysis using word2vec and glove embeddings. AI & society, pages\\n1–16, 2022.\\n[28] Paul Azunre. Transfer learning for natural language processing. Simon and Schuster, 2021.\\n[29] Yangyang Shi, Martha Larson, and Catholijn M Jonker. Recurrent neural network language model adaptation\\nwith curriculum learning. Computer Speech & Language, 33(1):136–154, 2015.\\n[30] Tomas Mikolov and Geoffrey Zweig. Context dependent recurrent neural network language model. In 2012\\nIEEE Spoken Language Technology Workshop (SLT), pages 234–239. IEEE, 2012.\\n[31] Aldin Kovaˇcevi´c and Dino Keˇco. Bidirectional lstm networks for abstractive text summarization. In Advanced\\nTechnologies, Systems, and Applications VI: Proceedings of the International Symposium on Innovative and\\nInterdisciplinary Applications of Advanced Technologies (IAT) 2021, pages 281–293. Springer, 2022.\\n[32] Nur Mohammad Fahad, Sadman Sakib, Mohaimenul Azam Khan Raiaan, and Md Saddam Hossain Mukta.\\nSkinnet-8: An efficient cnn architecture for classifying skin cancer on an imbalanced dataset. In 2023 In-\\nternational Conference on Electrical, Computer and Communication Engineering (ECCE), pages 1–6. IEEE,\\n2023.\\n[33] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim\\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the\\ngap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.\\n[34] Rajesh Kumar Yadav, Sahil Harwani, Satyendra Kumar Maurya, and Sachin Kumar. Intelligent chatbot using\\ngnmt, seq-2-seq techniques. In 2021 International Conference on Intelligent Technologies (CONIT), pages 1–5.\\nIEEE, 2021.\\n[35] Dieuwertje Luitse and Wiebke Denkena. The great transformer: Examining the role of large language models in\\nthe political economy of ai. Big Data & Society, 8(2):20539517211047734, 2021.\\n[36] M Onat Topal, Anil Bas, and Imke van Heerden. Exploring transformers in natural language generation: Gpt,\\nbert, and xlnet. arXiv preprint arXiv:2102.08036, 2021.\\n[37] Chiranjib Sur. Rbn: enhancement in language attribute prediction using global representation of natural language\\ntransfer learning technology like google bert. SN Applied Sciences, 2(1):22, 2020.\\n[38] Jordan J Bird, Anikó Ekárt, and Diego R Faria. Chatbot interaction with artificial intelligence: human data\\naugmentation with t5 and language transformer ensemble for text classification. Journal of Ambient Intelligence\\nand Humanized Computing, 14(4):3129–3144, 2023.\\n[39] Brady D Lund and Ting Wang. Chatting about chatgpt: how may ai and gpt impact academia and libraries?\\nLibrary Hi Tech News, 40(3):26–29, 2023.\\n[40] Benyamin Ghojogh and Ali Ghodsi. Attention mechanism, transformers, bert, and gpt: tutorial and survey. 2020.\\n[41] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are\\nunsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\\n[42] Katherine Abramski, Salvatore Citraro, Luigi Lombardi, Giulio Rossetti, and Massimo Stella. Cognitive network\\nscience reveals bias in gpt-3, gpt-3.5 turbo, and gpt-4 mirroring math anxiety in high-school students. Big Data\\nand Cognitive Computing, 7(3):124, 2023.\\n27'), Document(metadata={'source': './data/LLMS.pdf', 'page': 27}, page_content='A Review on LLM: Architectures, Applications, Open Issues and Challenges\\n[43] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.\\nMegatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint\\narXiv:1909.08053, 2019.\\n[44] Daniel Martin Katz, Michael James Bommarito, Shang Gao, and Pablo Arredondo. Gpt-4 passes the bar exam.\\nAvailable at SSRN 4389233, 2023.\\n[45] Xianrui Zheng, Chao Zhang, and Philip C Woodland. Adapting gpt, gpt-2 and bert language models for speech\\nrecognition. In 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 162–168.\\nIEEE, 2021.\\n[46] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\\nZettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint\\narXiv:1907.11692, 2019.\\n[47] Jianmo Ni, Gustavo Hernández Ábrego, Noah Constant, Ji Ma, Keith B Hall, Daniel Cer, and Yinfei Yang.\\nSentence-t5: Scalable sentence encoders from pre-trained text-to-text models. arXiv preprint arXiv:2108.08877,\\n2021.\\n[48] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4.\\narXiv preprint arXiv:2304.03277, 2023.\\n[49] Luciano Floridi and Massimo Chiriatti. Gpt-3: Its nature, scope, limits, and consequences. Minds and Machines,\\n30:681–694, 2020.\\n[50] Jesse Vig and Yonatan Belinkov. Analyzing the structure of attention in a transformer language model. arXiv\\npreprint arXiv:1906.04284, 2019.\\n[51] Alessia McGowan, Yunlai Gui, Matthew Dobbs, Sophia Shuster, Matthew Cotter, Alexandria Selloni, Mari-\\nanne Goodman, Agrima Srivastava, Guillermo A Cecchi, and Cheryl M Corcoran. Chatgpt and bard exhibit\\nspontaneous citation fabrication during psychiatry literature search. Psychiatry Research, 326:115334, 2023.\\n[52] Wei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao, Zhiwei Wang, Xin Jiang, ZhenZhang Yang, Kaisheng\\nWang, Xiaoda Zhang, et al. Pangu: Large-scale autoregressive pretrained chinese language models with\\nauto-parallel computation. arXiv preprint arXiv:2104.12369, 2021.\\n[53] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel Hesslow, Roman Castagné,\\nAlexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access\\nmultilingual language model. arXiv preprint arXiv:2211.05100, 2022.\\n[54] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew\\nPoulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint\\narXiv:2211.09085, 2022.\\n[55] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,\\nMona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint\\narXiv:2205.01068, 2022.\\n[56] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego\\nde Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large\\nlanguage models. arXiv preprint arXiv:2203.15556, 2022.\\n[57] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza\\nAlobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm:\\noutperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023.\\n[58] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a\\nlanguage model? arXiv preprint arXiv:2002.08910, 2020.\\n[59] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\\nlanguage models. arXiv preprint arXiv:2302.13971, 2023.\\n[60] Thanh Thi Nguyen, Campbell Wilson, and Janis Dalins. Fine-tuning llama 2 large language models for detecting\\nonline sexual predatory chats and abusive texts. arXiv preprint arXiv:2308.14683, 2023.\\n[61] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun\\nLiu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to train\\nmegatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022.\\n[62] Opher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. Jurassic-1: Technical details and evaluation. White\\nPaper. AI21 Labs, 1, 2021.\\n28'), Document(metadata={'source': './data/LLMS.pdf', 'page': 28}, page_content='A Review on LLM: Architectures, Applications, Open Issues and Challenges\\n[63] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides,\\nSarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights\\nfrom training gopher. arXiv preprint arXiv:2112.11446, 2021.\\n[64] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi\\nZheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022.\\n[65] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia\\nJin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint\\narXiv:2201.08239, 2022.\\n[66] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul\\nBarham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with\\npathways. arXiv preprint arXiv:2204.02311, 2022.\\n[67] Mohaimenul Azam Khan Raiaan, Kaniz Fatema, Inam Ullah Khan, Sami Azam, Md Rafi ur Rashid, Md Sad-\\ndam Hossain Mukta, Mirjam Jonkman, and Friso De Boer. A lightweight robust deep learning model gained\\nhigh accuracy in classifying a wide range of diabetic retinopathy images. IEEE Access, 2023.\\n[68] Inam Ullah Khan, Mohaimenul Azam Khan Raiaan, Kaniz Fatema, Sami Azam, Rafi ur Rashid, Saddam Hossain\\nMukta, Mirjam Jonkman, and Friso De Boer. A computer-aided diagnostic system to identify diabetic retinopathy,\\nutilizing a modified compact convolutional transformer and low-resolution images to reduce computation time.\\nBiomedicines, 11(6):1566, 2023.\\n[69] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi\\nZhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In\\nInternational Conference on Machine Learning, pages 5547–5569. PMLR, 2022.\\n[70] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor\\nLeahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive language model. arXiv\\npreprint arXiv:2204.06745, 2022.\\n[71] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming\\nXiong. Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint\\narXiv:2203.13474, 2022.\\n[72] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James\\nKeeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science,\\n378(6624):1092–1097, 2022.\\n[73] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet:\\nGeneralized autoregressive pretraining for language understanding. Advances in neural information processing\\nsystems, 32, 2019.\\n[74] Xingchen Song, Guangsen Wang, Zhiyong Wu, Yiheng Huang, Dan Su, Dong Yu, and Helen Meng. Speech-xlnet:\\nUnsupervised acoustic model pretraining for self-attention networks. arXiv preprint arXiv:1910.10387, 2019.\\n[75] Weizhou Shen, Junqing Chen, Xiaojun Quan, and Zhixian Xie. Dialogxl: All-in-one xlnet for multi-party\\nconversation emotion recognition. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35,\\npages 13789–13797, 2021.\\n[76] Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. Biogpt: generative\\npre-trained transformer for biomedical text generation and mining. Briefings in Bioinformatics, 23(6):bbac409,\\n2022.\\n[77] openai. openai, 2023. Accessed Sep 12, 2023.\\n[78] huggingface. huggingface, 2023. Accessed Sep 12, 2023.\\n[79] Google Cloud. Cloud natural language, 2023. Accessed Sep 12, 2023.\\n[80] azure. azure, 2023. Accessed Sep 12, 2023.\\n[81] IBM. Ibm watson natural language understanding, 2023. Accessed Sep 12, 2023.\\n[82] G Satyanarayana, J Bhuvana, and M Balamurugan. Sentimental analysis on voice using aws comprehend. In\\n2020 International Conference on Computer Communication and Informatics (ICCCI), pages 1–4. IEEE, 2020.\\n[83] Adam Kolides, Alyna Nawaz, Anshu Rathor, Denzel Beeman, Muzammil Hashmi, Sana Fatima, David Berdik,\\nMahmoud Al-Ayyoub, and Yaser Jararweh. Artificial intelligence foundation and pre-trained models: Fundamen-\\ntals, applications, opportunities, and social impacts. Simulation Modelling Practice and Theory, 126:102754,\\n2023.\\n29'), Document(metadata={'source': './data/LLMS.pdf', 'page': 29}, page_content='A Review on LLM: Architectures, Applications, Open Issues and Challenges\\n[84] Shashank Mohan Jain. Hugging face. In Introduction to Transformers for NLP: With the Hugging Face Library\\nand Models to Solve Problems, pages 51–67. Springer, 2022.\\n[85] Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Haojun Huang,\\nWei Ye, Xiubo Geng, et al. On the robustness of chatgpt: An adversarial and out-of-distribution perspective.\\narXiv preprint arXiv:2302.12095, 2023.\\n[86] Shan Chen, Yingya Li, Sheng Lu, Hoang Van, Hugo JWL Aerts, Guergana K Savova, and Danielle S Bit-\\nterman. Evaluation of chatgpt family of models for biomedical reasoning and classification. arXiv preprint\\narXiv:2304.02496, 2023.\\n[87] Hanyao Huang, Ou Zheng, Dongdong Wang, Jiayi Yin, Zijin Wang, Shengxuan Ding, Heng Yin, Chuan Xu,\\nRenjie Yang, Qian Zheng, et al. Chatgpt for shaping the future of dentistry: the potential of multi-modal large\\nlanguage model. International Journal of Oral Science, 15(1):29, 2023.\\n[88] Vera Sorin, Eyal Klang, Miri Sklair-Levy, Israel Cohen, Douglas B Zippel, Nora Balint Lahat, Eli Konen, and\\nYiftach Barash. Large language model (chatgpt) as a support tool for breast tumor board. NPJ Breast Cancer,\\n9(1):44, 2023.\\n[89] Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and\\nDaniel Shu Wei Ting. Large language models in medicine. Nature medicine, pages 1–11, 2023.\\n[90] Diane M Korngiebel and Sean D Mooney. Considering the possibilities and pitfalls of generative pre-trained\\ntransformer 3 (gpt-3) in healthcare delivery. NPJ Digital Medicine, 4(1):93, 2021.\\n[91] Luigi De Angelis, Francesco Baglivo, Guglielmo Arzilli, Gaetano Pierpaolo Privitera, Paolo Ferragina, Al-\\nberto Eugenio Tozzi, and Caterina Rizzo. Chatgpt and the rise of large language models: the new ai-driven\\ninfodemic threat in public health. Frontiers in Public Health, 11:1166120, 2023.\\n[92] Malik Sallam. Chatgpt utility in healthcare education, research, and practice: systematic review on the promising\\nperspectives and valid concerns. In Healthcare, volume 11, page 887. MDPI, 2023.\\n[93] Marco Cascella, Jonathan Montomoli, Valentina Bellini, and Elena Bignami. Evaluating the feasibility of chatgpt\\nin healthcare: an analysis of multiple clinical and research scenarios. Journal of Medical Systems, 47(1):33,\\n2023.\\n[94] Tiffany H Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, Maria\\nMadriaga, Rimel Aggabao, Giezel Diaz-Candido, James Maningo, et al. Performance of chatgpt on usmle:\\nPotential for ai-assisted medical education using large language models. PLoS digital health, 2(2):e0000198,\\n2023.\\n[95] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\\nGao, and Hoifung Poon. Domain-specific language model pretraining for biomedical natural language processing.\\nACM Transactions on Computing for Healthcare (HEALTH), 3(1):1–23, 2021.\\n[96] Zeljko Kraljevic, Dan Bean, Anthony Shek, Rebecca Bendayan, Harry Hemingway, and Joshua Au. Foresight-\\ngenerative pretrained transformer (gpt) for modelling of patient timelines using ehrs.\\n[97] Luisa Mich and Roberto Garigliano. Chatgpt for e-tourism: a technological perspective. Information Technology\\n& Tourism, pages 1–12, 2023.\\n[98] Xinli Yu, Zheng Chen, Yuan Ling, Shujing Dong, Zongyi Liu, and Yanbin Lu. Temporal data meets llm–\\nexplainable financial time series forecasting. arXiv preprint arXiv:2306.11025, 2023.\\n[99] Guilherme Francisco Frederico. Chatgpt in supply chains: Initial evidence of applications and potential research\\nagenda. Logistics, 7(2):26, 2023.\\n[100] Adam Sobieszek and Tadeusz Price. Playing games with ais: the limits of gpt-3 and similar large language\\nmodels. Minds and Machines, 32(2):341–364, 2022.\\n[101] Muhammad Usman Hadi, Rizwan Qureshi, Abbas Shah, Muhammad Irfan, Anas Zafar, Muhammad Bilal\\nShaikh, Naveed Akhtar, Jia Wu, Seyedali Mirjalili, et al. Large language models: A comprehensive survey of its\\napplications, challenges, limitations, and future prospects. 2023.\\n[102] Chung Kwan Lo. What is the impact of chatgpt on education? a rapid review of the literature. Education\\nSciences, 13(4):410, 2023.\\n[103] Yogesh K Dwivedi, Nir Kshetri, Laurie Hughes, Emma Louise Slade, Anand Jeyaraj, Arpan Kumar Kar,\\nAbdullah M Baabdullah, Alex Koohang, Vishnupriya Raghavan, Manju Ahuja, et al. “so what if chatgpt wrote\\nit?” multidisciplinary perspectives on opportunities, challenges and implications of generative conversational ai\\nfor research, practice and policy. International Journal of Information Management, 71:102642, 2023.\\n30'), Document(metadata={'source': './data/LLMS.pdf', 'page': 30}, page_content='A Review on LLM: Architectures, Applications, Open Issues and Challenges\\n[104] Mingyu Zong and Bhaskar Krishnamachari. A survey on gpt-3. arXiv preprint arXiv:2212.00857, 2022.\\n[105] Runjie Zhu, Xinhui Tu, and Jimmy Xiangji Huang. Utilizing bert for biomedical and clinical text mining. In\\nData analytics in biomedical engineering and healthcare, pages 73–103. Elsevier, 2021.\\n[106] Kexin Huang, Abhishek Singh, Sitong Chen, Edward T Moseley, Chih-Ying Deng, Naomi George, and Charlotta\\nLindvall. Clinical xlnet: modeling sequential clinical notes and predicting prolonged mechanical ventilation.\\narXiv preprint arXiv:1912.11975, 2019.\\n[107] Jipeng Zhang, Lei Wang, Roy Ka-Wei Lee, Yi Bin, Yan Wang, Jie Shao, and Ee-Peng Lim. Graph-to-tree\\nlearning for solving math word problems. In Proceedings of the 58th Annual Meeting of the Association for\\nComputational Linguistics, pages 3928–3937, 2020.\\n[108] Xiang Dai, Sarvnaz Karimi, Ben Hachey, and Cecile Paris. Cost-effective selection of pretraining data: A case\\nstudy of pretraining bert on social media. arXiv preprint arXiv:2010.01150, 2020.\\n[109] Som Biswas. The function of chat gpt in social media: According to chat gpt. Available at SSRN 4405389, 2023.\\n[110] Ruoling Peng, Kang Liu, Po Yang, Zhipeng Yuan, and Shunbao Li. Embedding-based retrieval with llm for\\neffective agriculture information extracting from unstructured data. arXiv preprint arXiv:2308.03107, 2023.\\n[111] Som Biswas. Importance of chat gpt in agriculture: According to chat gpt. Available at SSRN 4405391, 2023.\\n[112] Telmo Pires, Eva Schlinger, and Dan Garrette. How multilingual is multilingual bert? arXiv preprint\\narXiv:1906.01502, 2019.\\n[113] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco\\nGuzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual\\nrepresentation learning at scale. arXiv preprint arXiv:1911.02116, 2019.\\n[114] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller,\\nfaster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.\\n[115] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert:\\nA lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.\\n[116] Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. Man is to computer\\nprogrammer as woman is to homemaker? debiasing word embeddings. Advances in neural information\\nprocessing systems, 29, 2016.\\n[117] Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu,\\nand Bill Dolan. Dialogpt: Large-scale generative pre-training for conversational response generation. arXiv\\npreprint arXiv:1911.00536, 2019.\\n[118] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\\nAdvances in neural information processing systems, 33:1877–1901, 2020.\\n[119] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\\nBiobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics,\\n36(4):1234–1240, 2020.\\n[120] Liming Zhu, Xiwei Xu, Qinghua Lu, Guido Governatori, and Jon Whittle. Ai and ethics—operationalizing\\nresponsible ai. Humanity Driven AI: Productivity, Well-being, Sustainability and Partnership, pages 15–33,\\n2022.\\n[121] Inge Molenaar, Susanne de Mooij, Roger Azevedo, Maria Bannertd, Sanna Järveläe, and Dragan Gaševi ´cf.\\nMeasuring self-regulated learning and the role of ai: Five years of research using multimodal multichannel data.\\nComputers in Human Behavior, page 107540, 2022.\\n[122] Matthias C Rillig, Marlene Ågerstrand, Mohan Bi, Kenneth A Gould, and Uli Sauerland. Risks and benefits of\\nlarge language models for the environment. Environmental Science & Technology, 57(9):3464–3466, 2023.\\n[123] Bowen Liu, Boao Xiao, Xutong Jiang, Siyuan Cen, Xin He, Wanchun Dou, et al. Adversarial attacks on large\\nlanguage model-based system and mitigating strategies: A case study on chatgpt. Security and Communication\\nNetworks, 2023, 2023.\\n[124] Zhongxiang Sun. A short survey of viewing large language models in legal aspect. arXiv preprint\\narXiv:2303.09136, 2023.\\n[125] Yu Meng, Martin Michalski, Jiaxin Huang, Yu Zhang, Tarek Abdelzaher, and Jiawei Han. Tuning language\\nmodels as training data generators for augmentation-enhanced few-shot learning. In International Conference on\\nMachine Learning, pages 24457–24477. PMLR, 2023.\\n31'), Document(metadata={'source': './data/LLMS.pdf', 'page': 31}, page_content='A Review on LLM: Architectures, Applications, Open Issues and Challenges\\n[126] Steven Fincke, Shantanu Agarwal, Scott Miller, and Elizabeth Boschee. Language model priming for cross-\\nlingual event extraction. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages\\n10627–10635, 2022.\\n[127] Neel Jain, Khalid Saifullah, Yuxin Wen, John Kirchenbauer, Manli Shu, Aniruddha Saha, Micah Goldblum,\\nJonas Geiping, and Tom Goldstein. Bring your own data! self-supervised evaluation for large language models.\\narXiv preprint arXiv:2306.13651, 2023.\\n32')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_file_path=\"./data/LLMS.pdf\"\n",
    "\n",
    "pdf_loader=PyPDFLoader(\n",
    "    pdf_file_path\n",
    ")\n",
    "\n",
    "pdf_data=pdf_loader.load()\n",
    "\n",
    "print(pdf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://youngreadersfoundation.org/importance-of-reading/', 'title': 'Importance of reading | Young Readers Foundation', 'description': 'There are many reasons why reading is good for everyone and especially necessary for young people. Reading is an exercise for the mind. It helps kids calm down…', 'language': 'en-US'}, page_content='\\n\\n\\n\\n\\n\\n\\nImportance of reading | Young Readers Foundation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\nConcern\\nImportance of reading\\nWhat we do\\nGet involved\\n\\nDonate\\n\\n\\nNews\\nAbout\\nContact\\nEN\\n\\nNL\\n\\n\\n \\n\\n\\n\\n\\n\\nSelect Page\\n\\n\\n  \\n \\n\\n\\n\\n \\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe Importance Of Reading\\nWhy we should read books every day\\n\\xa0\\n\\xa0\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\nReading is an exercise for the mind. It helps kids calm down and relax, opening doors of new knowledge to enlighten their minds. Kids who read grow up to have better cognitive skills.\\nReading is good for everyone, not only children or young adults. On the internet you will find many lists with up to 30 reasons why reading is important. Here I limit myself to  15 thoroughly substantiated reasons.\\n\\n \\n \\n \\n \\n\\n\\n\\n\\n1.\\n\\n \\n \\n\\n\\nReading improves vocabulary\\nEven as adults, when we read, we come across many new words we never really heard of. And we learn from this. As you read, you come across new words, phrases and writing styles.\\nThis is even more so for young people. Children sometimes stumble over their words, do not know how to pronounce them or what they mean. By reading, young people encounter new words more frequently and sometimes repetitively and therefore can see them better in their context. If you then pay attention to the pronunciation as a parent, these children will be better prepared for school.\\n\\n \\n \\n \\n\\n\\n\\n2.\\n\\n \\n \\n\\n\\nBetter comprehension\\nKids who are encouraged to read at an early age have better comprehension of things around them. They develop smart thinking abilities and are more receptive to creativity and ideas that other kids their age lack. As a result, they grow up to be a good deal more intelligent and aware of their surroundings than kids who don’t read.\\nThe more you read, the more imaginative you become. Whenever you read a fiction book, it takes you another world. In the new world, your imagination works at its best as you try to see things in your own mind.\\n\\n \\n \\n \\n\\n\\n\\n3.\\n\\n \\n \\n\\n\\nDevelops critical thinking skills\\nOne of the primary benefits of reading books is its ability to develop critical thinking skills. For example, reading a mystery novel sharpens your mind. What elements are there in a story to make this or that conclusion. Or if a book is non-fiction you will sometimes ask yourself if the author is right. Critical thinking skills are crucial when it comes to making important day to day decisions. Reading requires an individual to think and process information in a way that watching television can’t. The more you read, the deeper your understanding becomes about what you’re reading and its application.\\n\\n \\n \\n \\n\\n\\n\\n4.\\n\\n \\n \\n\\n\\nImproves memory\\nEvery time you read a book, you have to remember the setting of the book, the characters, their backgrounds, their history, their personalities, the sub-plots and so much more. As your brain learns to remember all this, your memory becomes better. What’s more, with every new memory you create, you create new pathways and this strengthens the existing ones.\\n\\n \\n \\n \\n\\n\\n\\n5.\\n\\n \\n \\n\\n\\nImproves results at school\\nKids who indulge in reading book and learning new things do better at school. They are more creative, open to new ideas, and develop empathy for others. For instance, kids who read about heroes idolize them, kids who love reading anatomy books dream of becoming a doctor, etc. They learn to empathize with characters in the books and want to be like them. Not only that, they learn valuable life lessons such as helping others and being kind. Moral codes such as goods things will be appreciated and evils punished take root in their minds too, as a result of which they learn to stay away from trouble.\\n\\n \\n \\n \\n\\n\\n\\n6.\\n\\n \\n \\n\\n\\nImproves analytical skills\\nFiguring out how the story was going to end before finishing the book means you utilized your analytical skills. Reading allows your thinking skills to become more developed in the sense that you consider all aspects.\\n\\n \\n \\n \\n\\n\\n\\n7.\\n\\n \\n \\n\\n\\nBuilds confidence\\nIn a world where competition in every walk of life prevails, we need to build a child’s personality as to have considerable confidence in themselves. Kids who lack confidence in their early stages often grow up to be shy, and at times suicidal, since they develop a victim mentality owing to the lack of confidence in their own self. They find it hard to face even the smallest of challenges life throws at them, instead simply giving up. Reading books sharpens many skills and all together they’ll build confidence.\\n\\n \\n \\n \\n\\n\\n\\n8.\\n\\n \\n \\n\\n\\nHelps you socialize\\nWe can always share whatever we have read with our family, friends and colleagues. All this increases our ability to socialize. Humans are social beings and in the world of smartphones, we are losing our ability to socialize. However, reading had led to the formation of book clubs and other forums where we get a chance to share and interact with others.\\n\\n \\n \\n \\n\\n\\n\\n9.\\n\\n \\n \\n\\n\\nBroadens horizons\\nBy reading books, you get a glimpse of other cultures and places. Books expand your horizons, letting you see other countries, other people and so many other things you have never seen or imagined. It’s the perfect way to visit a strange country in your mind.\\nWhen we open a book while sitting in the comfort of our rooms, like time travelling, we transport our imaginations to a world purely based on the imaginations of the author. We learn about everything they wants u to know, see the world through their eyes and their perspective, learn about new people, discover their traditions, cultures and all that makes them unique and unforgettable.\\n\\n \\n \\n \\n\\n\\n\\n10.\\n\\n \\n \\n\\n\\nImproves writing skills\\nReading a well-written book affects your ability to become a better writer. Just like artists influence others, so do writers. Many successful authors gained their expertise by reading the works of others.\\nKids who learn to read also tend to develop better writing skills. The reason: they have been introduced to a world where words are their main weapon and they are free to shoot out. Literally! Parents must try to develop an interest for writing. Kids with good writing skills don’t fall victim to cramming and can express themselves more candidly through their words.\\n\\n \\n \\n \\n\\n\\n\\n11.\\n\\n \\n \\n\\n\\nImproves focus and concentration\\nIn our internet-crazed world, attention is drawn in a million different directions at once as we multi-task through every day. In a single 5-minute span, the average person will divide their time between working on a task, checking email, chatting with a couple of people (via gchat, skype, etc.), keeping an eye on twitter, monitoring their smartphone, and interacting with co-workers. This type of ADD-like behavior causes stress levels to rise, and lowers our productivity. When you read a book, all of your attention is focused on the story—the rest of the world just falls away, and you can immerse yourself in every fine detail you’re absorbing. Try reading for 15-20 minutes before work (i.e. on your morning commute, if you take public transit), and you’ll be surprised at how much more focused you are once you get to the office or school.\\n\\n \\n \\n \\n\\n\\n\\n12.\\n\\n \\n \\n\\n\\nMakes you more empathetic\\nAccording to studies, losing yourself in books, especially fiction, might increase your empathy. In a study conducted in the Netherlands, researchers showed that people who were “emotionally transported” by a work of fiction experienced a boost in empathy. By reading a book, you become part of the story and feel the pain and other emotions of the characters. This in turn allows your mind to become more aware of how different things affect other people. Eventually, this improves your ability to emphasize with other people.\\n\\n \\n \\n \\n\\n\\n\\n13.\\n\\n \\n \\n\\n\\nIt develops emotions\\nWhen you read a book, you are on the receiving end of knowledge. The sender, the writer is delivering a message, imparting something of value, a fact, an opinion, a view or at the very least an emotion. They are inviting you into their own psyche and hoping that you will care enough to listen and respond to it.\\nSo it won’t be wrong to say that reading actually flexes emotions. It builds a connection between the reader and the writer you have never met or known before. Even if you disagree with what they are delivering, you get to know them, and you connect to them on an emotional level.\\n\\n \\n \\n \\n\\n\\n\\n14.\\n\\n \\n \\n\\n\\nReaders are leaders\\nAlthough not definitively proved, but almost all great leaders were readers. One reason they are respected and known for their wisdom is because they develop a healthy reading habit. For centuries, reading has been the source of inspiration, growth and new ideas. It is a valuable investment in one’s own personality with uncountable and long-lasting benefits. If you want your child to become one, you need to encourage him to read. It will keep his mind healthy and productive. Only then they will be able to impact the world in a better way.\\n\\n\\n \\n \\n \\n\\n\\n\\n15.\\n\\n \\n \\n\\n\\nLearn at your own pace\\nAnother benefit of reading a book is that you learn at your own pace. Since you have the book all the time, you can always go back to a section you feel you don’t understand. You can re-read a chapter as many times as you wish, without worry that you will miss out a section. If it’s a self-help book, you can tackle one issue at a time. Once you handle one problem, then you can move to the next issue whenever you feel you’re ready. Everything is done at your own pace and most importantly, your mind is free to interpret things the way you feel.\\n\\n \\n \\n \\n \\n\\n\\n\\n\\nReading books also reduces stress, helps you sleep better, improves health, develops your imagination and above all: it is just fun to do.\\nReading has a tremendous effect in fueling all aspects of our personality and enhancing our linguistic prowess. In fact, it wouldn’t be wrong to say that the entirety of human life depends on it. Whatever we grow up to become in our lives, no matter where we stand, reading has somehow shaped it.\\n\\xa0\\n\\n \\n\\nsource listings:\\n23 Reasons Why You Need To Encourage Kids To Read  by Serious Reading\\nhttps://seriousreading.com/blog/1001-23-reasons-why-you-need-to-encourage-kids-to-read.html\\n30 Reasons to Read Books by Serious Reading\\nhttps://seriousreading.com/blog/283-30-reasons-to-read-books.html\\n12 Reasons Why You Should Read More Books by Georgette Rivera\\nhttps://www.theodysseyonline.com/12-reasons-should-read\\n10 Benefits of Reading: Why You Should Read Every Day  by Lana Winter-Hébert\\nhttps://www.lifehack.org/articles/lifestyle/10-benefits-reading-why-you-should-read-everyday.html\\n\\n \\n \\n\\n\\n\\n \\n \\n \\n\\n\\n\\n\\nSITEMAP\\nHome\\nConcern\\nImportance of reading\\nWhat we do\\nGet involved\\nDonate\\nNews\\nAbout\\n\\n \\n \\n\\n\\nUSAGE\\nBy using this website you agree with our privacy policy and the use of cookies.\\nPrivacy policy\\nCookie usage\\n\\n \\n \\n\\n\\nEXTERNAL LINKS\\nYRF on Fundrazr\\nMaryDes Designs Ltd.\\n\\n \\n\\nFacebook\\n\\nTwitter\\n\\n \\n \\n \\n  \\n  \\n \\n \\n\\n\\n\\n   \\n \\n\\n\\n(C) 2018 Young Readers Foundation  \\n\\n \\n \\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nShare This\\n\\n\\n\\n\\n\\nFacebook\\n\\n\\n\\n\\n\\nTwitter\\n\\n\\n\\n\\n\\nGoogle+\\n\\n\\n\\n\\n\\nLinkedIn\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "web_loader = WebBaseLoader(\"https://youngreadersfoundation.org/importance-of-reading/\")\n",
    "web_data=web_loader.load()\n",
    "print(web_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the loaded data into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "all_documents = json_data + csv_data + pdf_data + web_data\n",
    " \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "split_docs = text_splitter.split_documents(all_documents)\n",
    "\n",
    "print(len(split_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Source and working Embedding techniques in Langchain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adeel\\AppData\\Local\\Temp\\ipykernel_4952\\2218368237.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  hf_embed = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "c:\\Users\\adeel\\anaconda3\\envs\\rag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "hf_embed = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector = hf_embed.embed_query(\"This is a sample text.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "hf_embed = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vector = hf_embed.embed_query(\"This is a sample text. my name is Adeel Ahmed ..\")\n",
    "\n",
    "print(len(vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Storing databases .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most Commonly used Faiss and Chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "split_docs = [str(doc) for doc in split_docs]\n",
    "\n",
    "\n",
    "db = FAISS.from_texts(split_docs, hf_embed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='35ae76d5-b568-4e8b-afa7-b2fc24154f52', metadata={}, page_content=\"page_content='What is the capital of France?: What is the primary purpose of the corpus callosum?\\nThe capital of France is Paris.: The primary purpose of the corpus callosum is to connect the left and right hemispheres of the brain.' metadata={'source': './data/FAQs.csv', 'row': 156}\"), Document(id='45b45144-d5ed-47f8-a2e8-deeb4e5963a5', metadata={}, page_content=\"page_content='What is the capital of France?: What is the primary function of the temporal lobe?\\nThe capital of France is Paris.: The primary function of the temporal lobe is to process auditory information and is involved in memory.' metadata={'source': './data/FAQs.csv', 'row': 150}\")]\n"
     ]
    }
   ],
   "source": [
    "results = db.similarity_search(\"What is the primary purpose of the corpus callosum?\", k=2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "db = Chroma.from_texts(split_docs, hf_embed)\n",
    "results = db.similarity_search(\"What is the primary purpose of the corpus callosum?\", k=2)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now using LLMS to fine tune your answer and get the accorate result form the resources "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv('API_KEY')\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0,\n",
    "    api_key=api_key\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just checking LLMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"ML stands for Machine Learning, which is a subset of Artificial Intelligence (AI). It's a field of study that focuses on developing algorithms and statistical models that enable computers to learn from data, without being explicitly programmed.\\n\\nIn traditional programming, a computer is given a set of instructions to perform a specific task. In contrast, machine learning allows a computer to learn from experience and improve its performance on a task over time, based on the data it receives.\\n\\nThere are several key aspects of machine learning:\\n\\n1. **Data**: Machine learning algorithms rely on large datasets to learn from. The quality and quantity of the data are crucial for the algorithm's performance.\\n2. **Algorithms**: Machine learning algorithms are designed to analyze the data and make predictions or decisions based on it. Examples of machine learning algorithms include decision trees, neural networks, and clustering.\\n3. **Model**: A machine learning model is a mathematical representation of the relationships between the input data and the desired output. The model is trained on the data and can be used to make predictions or decisions.\\n4. **Training**: The process of training a machine learning model involves feeding it with data and adjusting the model's parameters to minimize the error between the predicted output and the actual output.\\n5. **Testing**: Once the model is trained, it's tested on a separate dataset to evaluate its performance and accuracy.\\n\\nMachine learning has many applications, including:\\n\\n1. **Image recognition**: Machine learning algorithms can be trained to recognize objects, people, and patterns in images.\\n2. **Natural language processing**: Machine learning can be used to analyze and generate human language, such as text and speech.\\n3. **Predictive analytics**: Machine learning can be used to predict future events, such as stock prices or customer behavior.\\n4. **Robotics**: Machine learning can be used to control robots and enable them to learn from experience.\\n5. **Recommendation systems**: Machine learning can be used to recommend products or services based on a user's behavior and preferences.\\n\\nSome of the key benefits of machine learning include:\\n\\n1. **Improved accuracy**: Machine learning algorithms can learn from data and improve their performance over time.\\n2. **Increased efficiency**: Machine learning can automate many tasks, freeing up human resources for more complex and creative work.\\n3. **Enhanced decision-making**: Machine learning can provide insights and recommendations that can inform business decisions.\\n\\nHowever, machine learning also has some challenges and limitations, including:\\n\\n1. **Data quality**: Machine learning algorithms require high-quality data to learn from.\\n2. **Bias and fairness**: Machine learning algorithms can perpetuate biases and unfairness if the data is biased.\\n3. **Explainability**: Machine learning models can be difficult to interpret and understand.\\n4. **Security**: Machine learning models can be vulnerable to attacks and data breaches.\\n\\nOverall, machine learning is a powerful tool that has the potential to transform many industries and aspects of our lives.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 588, 'prompt_tokens': 39, 'total_tokens': 627, 'completion_time': 0.784, 'prompt_time': 0.002369694, 'queue_time': 0.233828856, 'total_time': 0.786369694}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f42b4eca2d', 'finish_reason': 'stop', 'logprobs': None}, id='run-00cce71d-add6-464a-b701-531bffb7147a-0', usage_metadata={'input_tokens': 39, 'output_tokens': 588, 'total_tokens': 627})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is ML ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding db as retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a helpful and knowledgeable assistant tasked with providing CONCISE answers.\n",
    "\n",
    "Below is the context (documents) related to the user's query. \n",
    "Based on this context, answer the following question in a BRIEF and CLEAR manner.\n",
    "Focus only on the most important information and limit your response to 3-5 sentences.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Concise Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Create a PromptTemplate instance\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"input\"], template=prompt_template)\n",
    "\n",
    "# Create the document chain\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Create the retrieval chain\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer is a neural network architecture designed to handle sequential data effectively. It uses an attention method to determine global input-output dependencies, allowing for parallelization and efficient handling of long-range dependencies. This architecture replaced traditional recurrent or convolutional neural networks in many fields, particularly in Natural Language Processing (NLP). The Transformer model is built around the self-attention mechanism, enabling it to excel at various language tasks.\n"
     ]
    }
   ],
   "source": [
    "# Define your query\n",
    "query = \"What is Transformer?\"\n",
    "\n",
    "# Execute the chain\n",
    "result = retrieval_chain.invoke({\"input\": query})\n",
    "\n",
    "# Extract and print only the answer\n",
    "answer = result['answer']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the code in one shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.document_loaders import JSONLoader, PyPDFLoader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv('API_KEY')\n",
    "\n",
    "# Load and process documents (left as is from your code)\n",
    "csv_loader = CSVLoader(file_path=\"./data/FAQs.csv\")\n",
    "csv_data = csv_loader.load()\n",
    "\n",
    "json_loader = JSONLoader(\n",
    "    file_path=\"./data/FAQs.json\",\n",
    "    jq_schema='map({question, answer})',\n",
    "    text_content=False\n",
    ")\n",
    "json_data = json_loader.load()\n",
    "\n",
    "pdf_loader = PyPDFLoader(\"./data/LLMS.pdf\")\n",
    "pdf_data = pdf_loader.load()\n",
    "\n",
    "web_loader = WebBaseLoader(\"https://youngreadersfoundation.org/importance-of-reading/\")\n",
    "web_data = web_loader.load()\n",
    "\n",
    "# Combine all documents\n",
    "all_documents = json_data + csv_data + pdf_data + web_data\n",
    "\n",
    "# Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "split_docs = text_splitter.split_documents(all_documents)\n",
    "\n",
    "# Create embeddings\n",
    "hf_embed = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create vector store\n",
    "db = FAISS.from_documents(split_docs, hf_embed)\n",
    "\n",
    "# Initialize LLM with lower temperature for more concise responses\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0,  # Keep temperature low for factual responses\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "# Set up retriever with a lower k value to get fewer documents\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})  # Limit to top 3 most relevant documents\n",
    "\n",
    "# Create an improved prompt template that specifically asks for concise answers\n",
    "prompt_template = \"\"\"\n",
    "You are a helpful and knowledgeable assistant tasked with providing CONCISE answers.\n",
    "\n",
    "Below is the context (documents) related to the user's query. \n",
    "Based on this context, answer the following question in a BRIEF and CLEAR manner.\n",
    "Focus only on the most important information and limit your response to 3-5 sentences.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Concise Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Create a PromptTemplate instance\n",
    "prompt = PromptTemplate(input_variables=[\"context\", \"input\"], template=prompt_template)\n",
    "\n",
    "# Create the document chain\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Create the retrieval chain\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer is a neural network architecture designed to handle sequential data effectively. It uses an attention method to determine global input-output dependencies, allowing for parallelization and efficient handling of long-range dependencies. This architecture replaced traditional recurrent or convolutional neural networks in many fields, particularly in Natural Language Processing (NLP). The Transformer model is built around the self-attention mechanism, enabling it to excel at various language tasks.\n"
     ]
    }
   ],
   "source": [
    "# Define your query\n",
    "query = \"What is Transformer?\"\n",
    "\n",
    "# Execute the chain\n",
    "result = retrieval_chain.invoke({\"input\": query})\n",
    "\n",
    "# Extract and print only the answer\n",
    "answer = result['answer']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
